from pymongo import MongoClient
import pandas as pd
import dns.resolver
import certifi
from bson import ObjectId, errors
import config
import os
from datetime import datetime
import numpy as np

# DNS resolver and MongoDB connection setup
dns.resolver.default_resolver = dns.resolver.Resolver(configure=False)
dns.resolver.default_resolver.nameservers = ['8.8.8.8']

carrier_list = ["AF", "KL","DL", "MU", "TO", "HV"]

client = MongoClient(f'mongodb+srv://{config.mongo_pat}', tlsCAFile=certifi.where())

# MongoDB collections
db = client['legacy-api-management']
col_items = db["items"]
col_bills = db["bills"]
col_users = db["users"]
col_soc = db["societies"]
col_billings = db["billings"]

airports_df = pd.read_csv("const/Air Airports and cities codes - O&D  2024.csv", delimiter=";")
rail_metro_df = pd.read_csv("const/O&D Metropolis rail 2024 v In&Out.csv",delimiter=";")
rail_euro_df = pd.read_csv("const/O&D European Rail 2024 v In&Out.csv",delimiter=";")

current_date = datetime.now()
current_year = current_date.year
last_month = current_date.month - 1 if current_date.month > 1 else 12
last_month_name = datetime(1900, last_month, 1).strftime('%B')
if last_month == 12:
    current_year -= 1  # Adjust the year if the last month is December from the previous year

def get_soc(id_soc):
    result = col_soc.find_one({"_id": ObjectId(f"{id_soc}")},
                              {"name": 1, "settings.flight.bluebizz": 1, "_id": 0})

    name = result.get("name", None)
    bluebizz = result.get("settings", {}).get("flight", {}).get("bluebizz", None)
    bluebizz_value = bluebizz if bluebizz else "Bluebizz"

    osi = bluebizz_value
    name_orga = name

    return osi,name_orga

def get_bills(id_soc,start_date):
    import pandas as pd
    from pymongo import MongoClient

    # D√©finir les crit√®res de filtrage
    filter_criteria = {
        "societyId": f"{id_soc}",
        "createdAt": {"$gte": start_date},
        "type": {"$in": ["receipt", "credit", "unitary"]}
    }

    # Requ√™te MongoDB pour extraire les informations des lignes
    projection = {
        "type": 1,  # Projeter 'type' √† la racine
        "lines.itemId": 1,
        "lines.label": 1,
        "lines.type": 1,
        "lines.price.amount": 1,
        "lines.userIds": 1,
        "_id": 0  # Exclure le champ _id des r√©sultats
    }

    # Effectuer la requ√™te
    documents = col_bills.find(filter_criteria, projection)

    # Liste pour stocker les r√©sultats
    data = []

    # Extraire les r√©sultats et aplatir les valeurs des lignes
    for doc in documents:
        doc_type = doc.get("type")  # R√©cup√©rer le 'type' √† la racine
        for line in doc.get('lines', []):
            # Extraire les valeurs des sous-documents
            item_data = {
                "ITEM_ID": line.get("itemId"),
                "type_bill": doc_type,  # Associer le type du document √† chaque ligne
                "label": line.get("label"),
                "type": line.get("type"),
                "TOTAL_BILLED": line.get("price", {}).get("amount"),
                "userIds": ', '.join([str(user_id) for user_id in line.get("userIds", [])])
            }
            data.append(item_data)

    # Convertir la liste en DataFrame
    df = pd.DataFrame(data)

    df.to_csv(f"csv/base/bills_{id_soc}.csv")
    # Afficher le DataFrame
    print(df)

################ üõ´FLIGHT üõ´ #####################

def fetch_flight_details(item_id):
    # Chercher l'item dans la collection MongoDB
    item = col_items.find_one({"id": item_id})
    print(item_id)
    if item and item.get('type') == 'flight':
        trips = item.get('detail', {}).get('trips', [])
        all_legs_details = []

        # Boucle sur tous les trips
        for trip_index, trip in enumerate(trips):
            legs = trip.get('legs', [])

            if legs:
                # Si 'legs' existe, traiter chaque leg
                for leg_index, leg in enumerate(legs):
                    leg_details = {
                        f"{leg_index}_ori_city": leg.get('departure', {}).get('city', ''),
                        f"{leg_index}_des_city": leg.get('arrival', {}).get('city', ''),
                        f"{leg_index}_ori_country": leg.get('departure', {}).get('country', ''),
                        f"{leg_index}_des_country": leg.get('arrival', {}).get('country', ''),
                        f"{leg_index}_governing_carrier": leg.get('governingCarrier', '')
                    }
                    all_legs_details.append(leg_details)
            else:
                # Si 'legs' n'existe pas, utiliser les chemins alternatifs
                leg_details = {
                    f"{trip_index}_ori_city": trip.get('Dep', {}).get('IATALocationCode', ''),
                    f"{trip_index}_des_city": trip.get('Arrival', {}).get('IATALocationCode', ''),
                    f"{trip_index}_ori_country": "NDC",  # Valeur par d√©faut
                    f"{trip_index}_des_country": "NDC",  # Valeur par d√©faut
                    f"{trip_index}_governing_carrier": trip.get('OperatingCarrierInfo', {}).get('CarrierDesigCode', '')
                }
                all_legs_details.append(leg_details)

        # Fusionner tous les d√©tails des legs dans un seul dictionnaire
        combined_leg_details = {}
        for leg_detail in all_legs_details:
            combined_leg_details.update(leg_detail)

        return combined_leg_details
    return {}

def concatenate_cities(row):
    number_of_legs = row['NB_LEGS']

    # Si le nombre de legs est sup√©rieur √† 3, renvoyer "check"
    if number_of_legs > 3:
        return "check"

    # Si le nombre de legs est inf√©rieur ou √©gal √† 2
    ori_city = row.get('0_ori_city', '')
    des_city = row.get('0_des_city', '')
    ori_country = row.get('0_ori_country', '')
    des_country = row.get('0_des_country', '')

    # Si 'NDC' est dans '0_ori_country', on change les variables pour utiliser les codes NDC
    if ori_country == 'NDC':
        ori_city = row.get('0_Ori_NDC', '')
        des_city = row.get('0_Des_NDC', '')
        ori_country = row.get('0_Ori_country_NDC', '')
        des_country = row.get('0_Des_country_NDC', '')

    special_cities = ['PAR', 'BIO', 'BRU', 'FRA', 'GVA', 'LUX']

    # G√©rer le cas o√π PAR est dans ori_city ou des_city
    if 'PAR' in [ori_city, des_city]:
        if ori_city == 'PAR':
            ori_city, des_city = 'PAR', des_city
        elif des_city == 'PAR':
            ori_city, des_city = 'PAR', ori_city

        if number_of_legs <= 2:
            return f"{ori_city} {des_city}"
        else:
            return f"{ori_city} {des_city} !!!"

    # G√©rer les autres villes sp√©ciales (BIO, BRU, FRA, GVA, LUX)
    elif ori_city in special_cities or des_city in special_cities:
        # Trier par ordre alphab√©tique si une des villes sp√©ciales est pr√©sente
        sorted_cities = sorted([ori_city, des_city])
        if number_of_legs <= 2:
            return f"{sorted_cities[0]} {sorted_cities[1]}"
        else:
            return f"{sorted_cities[0]} {sorted_cities[1]} !!!"

    # G√©rer le cas o√π l'origine et la destination sont en France
    elif ori_country == 'FR' and des_country == 'FR':
        sorted_cities = sorted([ori_city, des_city])
        if number_of_legs <= 2:
            return f"{sorted_cities[0]} {sorted_cities[1]}"
        else:
            return f"{sorted_cities[0]} {sorted_cities[1]} !!!"

    # G√©rer le cas o√π l'origine et la destination ne sont pas en France
    elif ori_country != 'FR' and des_country != 'FR':
        sorted_cities = sorted([ori_city, des_city])
        if number_of_legs <= 2:
            return f"{sorted_cities[0]} {sorted_cities[1]}"
        else:
            return f"{sorted_cities[0]} {sorted_cities[1]} !!!"

    # G√©rer le cas o√π l'origine est en France et la destination est √† l'√©tranger
    elif ori_country == 'FR' and des_country != 'FR':
        if number_of_legs <= 2:
            return f"{ori_city} {des_city}"
        else:
            return f"{ori_city} {des_city} !!!"

    # G√©rer le cas o√π l'origine est √† l'√©tranger et la destination est en France
    elif ori_country != 'FR' and des_country == 'FR':
        if number_of_legs <= 2:
            return f"{des_city} {ori_city}"
        else:
            return f"{des_city} {ori_city} !!!"

    return ""

def concatenate_carriers(row):
    carrier_columns = [col for col in row.index if col.endswith('_governing_carrier')]
    return ' '.join([str(row[col]) for col in carrier_columns if row[col]])

def classify_airline(concat_carrier):
    if any(carrier in concat_carrier for carrier in carrier_list):
        return "AIR FRANCE"
    return "INDUSTRIE"

def lookup_airport_details(concatenated_city):
    # Find the matching row where 'O&D restitu√©' matches the given concatenated_city
    match_row = airports_df[airports_df['O&D restitu√©'] == concatenated_city]

    # If no match found, try to find in 'O&D Inbound'
    if match_row.empty:
        match_row = airports_df[airports_df['O&D Inbound'] == concatenated_city]

    if not match_row.empty:
        # Extract values for each field if a match is found
        haul_type = match_row['Haul type'].values[0]
        origin_area = match_row['Origin area'].values[0]
        annex_c = match_row['Annex C/ Out of Annex C 2023'].values[0]
        label_origin = match_row['Label cities of origin'].values[0]
        label_destination = match_row['Label cities of destination'].values[0]
        od_restitue = match_row['O&D restitu√©'].values[0]  # Add the 'O&D restitu√©' column value

        # Return all the extracted values
        return haul_type, origin_area, annex_c, label_origin, label_destination, od_restitue

    # Return empty strings if no match is found
    return "", "", "", "", "", ""

def lookup_ndc_code(city, primary_column, primary_code_column, secondary_column=None, secondary_code_column=None):
    # Chercher la ville dans le fichier CSV dans la colonne primaire (e.g., 'Airport origin code')
    code_row = airports_df[airports_df[primary_column] == city]

    if not code_row.empty:
        return code_row[primary_code_column].values[0]

    # Si aucune correspondance trouv√©e et les colonnes secondaires sont sp√©cifi√©es
    if secondary_column and secondary_code_column:
        code_row = airports_df[airports_df[secondary_column] == city]
        if not code_row.empty:
            return code_row[secondary_code_column].values[0]

    return ""

def get_items_flight(id_soc, start_date):
    filter_criteria = {
        "society._id": ObjectId(id_soc),
        "createdAt": {"$gte": start_date},
        "type": "flight",
        "statusHistory.to": "confirmed",
        "status": {"$in": ["confirmed", "cancelled"]}
    }

    projection = {
        "id": 1,
        "offline": 1,
        "createdAt": 1,
        "type": 1,
        "status": 1,
        "billingId": 1,
        "travelers.billingId": 1,
        "detail.trips": 1,
        "_id": 0
    }

    items = col_items.find(filter_criteria, projection)
    data = []

    for item in items:
        offline_status = "Offline" if item.get("offline") else "Online"
        item_type = item.get("type")
        item_createdAt = item.get("createdAt")
        item_status = item.get("status", "Unknown")

        billing_id = item.get("billingId")
        if not billing_id:
            travelers = item.get("travelers", [])
            billing_id = next((traveler.get("billingId") for traveler in travelers if traveler.get("billingId")), None)

        raison = "Unknown"
        if billing_id:
            try:
                billing_doc = col_billings.find_one({"_id": ObjectId(billing_id)}, {"raison": 1})
            except errors.InvalidId:
                billing_doc = col_billings.find_one({"_id": billing_id}, {"raison": 1})

            if billing_doc:
                raison = billing_doc.get("raison", "Unknown")

        trips = item.get("detail", {}).get("trips", [])
        number_of_legs = sum([len(trip.get("legs", [])) if trip.get("legs", []) else 1 for trip in trips])

        basic_data = {
            "ITEM_ID": item.get("id"),
            "IS_OFFLINE": offline_status,
            "CREATED_AT": item_createdAt,
            "TYPE": item_type,
            "STATUS": item_status,
            "NB_LEGS": number_of_legs,
            "BILLING_ID": billing_id,
            "ENTITY": raison
        }

        leg_details = fetch_flight_details(item.get("id"))
        combined_data = {**basic_data, **leg_details}
        data.append(combined_data)

    df = pd.DataFrame(data)

    # Interroger le fichier CSV pour les colonnes avec 'NDC' dans les pays
    for i in range(2):  # Pour les index 0 et 1 (0_ori_country, 1_ori_country, etc.)
        ori_country_col = f'{i}_ori_country'
        des_country_col = f'{i}_des_country'
        ori_city_col = f'{i}_ori_city'
        des_city_col = f'{i}_des_city'

        # Si "NDC" dans ori_country, chercher dans 'Airport origin code', sinon dans 'Airport destination code'
        df[f'{i}_Ori_NDC'] = df.apply(
            lambda row: lookup_ndc_code(row[ori_city_col],
                                        'Airport origin code',
                                        'Origin Code',
                                        'Airport destination code',
                                        'Destination Code') if row[ori_country_col] == 'NDC' else '', axis=1)

        # Si "NDC" dans des_country, chercher dans 'Airport destination code', sinon dans 'Airport origin code'
        df[f'{i}_Des_NDC'] = df.apply(
            lambda row: lookup_ndc_code(row[des_city_col],
                                        'Airport destination code',
                                        'Destination Code',
                                        'Airport origin code',
                                        'Origin Code') if row[des_country_col] == 'NDC' else '', axis=1)

        # Condition pour Ori_country_NDC (Nouveau code)
        df[f'{i}_Ori_country_NDC'] = df.apply(
            lambda row: lookup_ndc_code(row[ori_city_col],
                                        'Airport origin code',
                                        'Country code of Origine',
                                        'Airport destination code',
                                        'Country code of destination') if row[ori_country_col] == 'NDC' else '', axis=1)

        # Condition pour Des_country_NDC (Nouveau code)
        df[f'{i}_Des_country_NDC'] = df.apply(
            lambda row: lookup_ndc_code(row[des_city_col],
                                        'Airport destination code',
                                        'Country code of destination',
                                        'Airport origin code',
                                        'Country code of Origine') if row[des_country_col] == 'NDC' else '', axis=1)


    # Concat√©ner les colonnes qui terminent par 'governing_carrier' pour cr√©er CONCAT_carrier
    df['CONCAT_carrier'] = df.apply(concatenate_carriers, axis=1)

    # Cr√©er la colonne 'AIRLINE_GROUP' pour classifier en fonction de la pr√©sence d'un code de 'carrier_list'
    df['AIRLINE_GROUP'] = df['CONCAT_carrier'].apply(classify_airline)

    # Sauvegarder le DataFrame en CSV
    df.to_csv(f"csv/base/items_flight_{id_soc}.csv", index=False)

def merge_extract_flight(id_soc):
    # Charger les fichiers CSV de vol et fusionner
    df0 = pd.read_csv(f"csv/base/bills_{id_soc}.csv")
    df0 = df0[df0["type"] == "flight"]
    df1 = pd.read_csv(f"csv/base/items_flight_{id_soc}.csv")
    df = pd.merge(df0, df1, on='ITEM_ID', how='inner')

    # Convertir CREATED_AT en formats YYYY-MM et YYYY
    df['ISSUED_MONTH'] = pd.to_datetime(df['CREATED_AT']).dt.strftime('%Y%m')
    df['ODLIST'] = pd.to_datetime(df['CREATED_AT']).dt.strftime('%Y')

    # Appliquer la fonction concatenate_cities √† chaque ligne du DataFrame
    df['CONCATENATED_CITIES'] = df.apply(concatenate_cities, axis=1)

    # Rechercher les d√©tails suppl√©mentaires dans 'airports_df' pour 'CONCATENATED_CITIES'
    df[['HAUL_TYPE', 'ORIGIN_AREA', 'ANNEX_C', 'LABEL_ORIGIN', 'LABEL_DESTINATION','O&D RESTITUE']] = df.apply(
        lambda row: pd.Series(lookup_airport_details(row['CONCATENATED_CITIES'])), axis=1)

    columns_to_keep = [
        'ITEM_ID', 'type_bill', 'ISSUED_MONTH', 'ODLIST', 'IS_OFFLINE', 'TOTAL_BILLED',
        'NB_LEGS', 'CONCATENATED_CITIES', 'HAUL_TYPE', 'ORIGIN_AREA', 'ANNEX_C',
        'LABEL_ORIGIN', 'LABEL_DESTINATION', 'AIRLINE_GROUP','O&D RESTITUE'
    ]

    # S√©lectionner uniquement les colonnes sp√©cifi√©es
    df_filtered = df[columns_to_keep]

    # Trier le DataFrame par 'ODLIST'
    df_filtered = df_filtered.sort_values(by='ODLIST')

    # Sauvegarder le DataFrame filtr√© dans un fichier CSV
    df_filtered.to_csv(f'csv/res/flight/raw_flight_{id_soc}.csv', index=False)

    df_filtered = df_filtered[df_filtered['O&D RESTITUE'] != ""]
    df_filtered.to_csv(f'csv/res/flight/filtered_flight_{id_soc}.csv', index=False)

def group_flight(id_soc,iata,osi):
    # Charger le fichier CSV t√©l√©charg√©
    file_path = f'csv/res/flight/filtered_flight_{id_soc}.csv'
    df = pd.read_csv(file_path)

    # Groupement par les colonnes sp√©cifi√©es
    grouping_columns = [
        'O&D RESTITUE', 'ORIGIN_AREA', 'ANNEX_C', 'LABEL_ORIGIN',
        'LABEL_DESTINATION', 'ISSUED_MONTH', 'ODLIST', 'IS_OFFLINE'
    ]

    # Agr√©gation des donn√©es
    df_grouped_af = df.groupby(grouping_columns).agg(
        TOTAL_BILLED_AIR_FRANCE=('TOTAL_BILLED', lambda x: x[df['AIRLINE_GROUP'] == 'AIR FRANCE'].sum()),
        TOTAL_BILLED_INDUSTRIE=('TOTAL_BILLED', lambda x: x[(df['AIRLINE_GROUP'] == 'AIR FRANCE') | (df['AIRLINE_GROUP'] == 'INDUSTRIE')].sum()),
        NB_LEGS_AIR_FRANCE=('NB_LEGS', lambda x: x[df['AIRLINE_GROUP'] == 'AIR FRANCE'].sum()),
        NB_LEGS_INDUSTRIE=('NB_LEGS', lambda x: x[(df['AIRLINE_GROUP'] == 'AIR FRANCE') | (df['AIRLINE_GROUP'] == 'INDUSTRIE')].sum())
    ).reset_index()


    # fichier brut
    df_grouped = df.groupby(grouping_columns).agg({
        'TOTAL_BILLED': 'sum',
        'NB_LEGS': 'sum'}).reset_index()

    df_grouped.to_csv(f"csv/res/flight/grouped_flight_{id_soc}.csv")

    # fichier af

    df_grouped_af[['CODE ORIGINE', 'CODE DESTINATION']] = df_grouped_af['O&D RESTITUE'].str.split(' ', n=1, expand=True)

    # Renommer les colonnes
    df_grouped_af = df_grouped_af.rename(columns={
        "O&D RESTITUE" : "O&D",
        "ISSUED_MONTH": "DATE D'EMISSION",
        'ORIGIN_AREA': 'ZONE ORIGINE',
        'ANNEX_C': 'PERIMETRE',
        'LABEL_ORIGIN': 'LIBELLE ORIGINE',
        'LABEL_DESTINATION': 'LIBELLE DESTINATION',
        'IS_OFFLINE': 'TYPE DE VENTE',
        'TOTAL_BILLED_AIR_FRANCE': 'CA GROUPE AF KL',
        'NB_LEGS_AIR_FRANCE': 'NB O&D GROUPE AF KL',
        'TOTAL_BILLED_INDUSTRIE': 'CA TOTAL INDUSTRIE',
        'NB_LEGS_INDUSTRIE': 'NB O&D INDUSTRIE',
    })
    df_grouped_af['CA GROUPE AF KL'] = df_grouped_af['CA GROUPE AF KL'].round(0).astype(int)
    df_grouped_af['NB O&D GROUPE AF KL'] = df_grouped_af['NB O&D GROUPE AF KL'].round(0).astype(int)
    df_grouped_af['CA TOTAL INDUSTRIE'] = df_grouped_af['CA TOTAL INDUSTRIE'].round(0).astype(int)
    df_grouped_af['NB O&D INDUSTRIE'] = df_grouped_af['NB O&D INDUSTRIE'].round(0).astype(int)
    df_grouped_af['REFERENCE AF'] = osi
    df_grouped_af['RAISON SOCIALE'] = ''
    df_grouped_af['IATA EMETTEUR'] = iata

    # Connexion en base pour recup OIN et NAME
    result = col_soc.find_one({"_id": ObjectId(id_soc)})

    # Extraire les variables "name" et "settings.flight.bluebizz"
    name = result.get("name", "Unknown")  # Assigner une valeur par d√©faut si name est None
    bluebizz = result.get("settings", {}).get("flight", {}).get("bluebizz", "Bluebizz")  # Valeur par d√©faut
    bluebizz_value = bluebizz if bluebizz else "Bluebizz"

    print(name, bluebizz_value)

    # Remplir les colonnes du DataFrame avec les valeurs r√©cup√©r√©es
    df_grouped_af['REFERENCE AF'] = bluebizz_value
    df_grouped_af['RAISON SOCIALE'] = name

    # Trier les valeurs par "DATE D'EMISSION" et "O&D"
    df_grouped_af = df_grouped_af.sort_values(by=["DATE D'EMISSION", "O&D"], ascending=[False, True])

    # V√©rifier si les valeurs de 'O&D' de flight_df sont pr√©sentes dans 'O&D restitu√©' de airports_df
    df_grouped_af['O&D_match'] = df_grouped_af['O&D'].isin(airports_df['O&D restitu√©'])

    # Filtrer les lignes o√π 'O&D' a une correspondance dans airports_df
    filtered_grouped_af = df_grouped_af[df_grouped_af['O&D_match']].drop(columns=['O&D_match'])

    # Sauvegarder le DataFrame filtr√© dans un nouveau fichier CSV
    filtered_grouped_af.to_csv(f"csv/res/flight/grouped_flight_{id_soc}.csv", index=False)

    print(f"File saved: csv/res/flight/grouped_flight_{id_soc}.csv")

################ üöÖ TRAIN üöÖ #####################

def fetch_train_details(item_id):
    # Chercher l'item dans la collection MongoDB
    item = col_items.find_one({"id": item_id})
    print(item_id)

    if item and item.get('type') == 'train':
        journeys = item.get('detail', {}).get('journeys', [])
        nombre_leg = len(journeys)  # Nombre de voyages/jambes

        all_legs_details = []

        # Boucle sur tous les voyages (journeys)
        for journey_index, journey in enumerate(journeys):
            if journey.get('departure') and journey.get('arrival'):
                ori_locationId = journey['departure'].get('locationId', '')
                des_locationId = journey['arrival'].get('locationId', '')

                leg_details = {
                    f"{journey_index}_ori_name": journey['departure'].get('name', ''),
                    f"{journey_index}_des_name": journey['arrival'].get('name', ''),
                    f"{journey_index}_ori_locationId": ori_locationId,
                    f"{journey_index}_des_locationId": des_locationId,
                }
                all_legs_details.append(leg_details)
            else:
                # Si 'departure' et 'arrival' n'existent pas, chercher dans les 'segments'
                segments = journey.get('segments', [])
                for segment_index, segment in enumerate(segments):
                    ori_locationId = segment['departure'].get('locationId', '')
                    des_locationId = segment['arrival'].get('locationId', '')

                    leg_details = {
                        f"{segment_index}_ori_name": segment['departure'].get('name', ''),
                        f"{segment_index}_des_name": segment['arrival'].get('name', ''),
                        f"{segment_index}_ori_locationId": ori_locationId,
                        f"{segment_index}_des_locationId": des_locationId,
                    }
                    all_legs_details.append(leg_details)

        # Fusionner tous les d√©tails des segments dans un seul dictionnaire
        combined_leg_details = {}
        for leg_detail in all_legs_details:
            combined_leg_details.update(leg_detail)

        return combined_leg_details, nombre_leg

    return {}, 0

def get_items_train(id_soc, start_date):
    filter_criteria = {
        "society._id": ObjectId(id_soc),
        "createdAt": {"$gte": start_date},
        "type": "train",
        "statusHistory.to": "confirmed",
        "status": {"$in": ["confirmed", "cancelled"]}
    }

    projection = {
        "id": 1,
        "offline": 1,
        "createdAt": 1,
        "type": 1,
        "status": 1,
        "billingId": 1,
        "travelers.billingId": 1,
        "detail.trips": 1,
        "_id": 0
    }

    items = col_items.find(filter_criteria, projection)
    data = []

    for item in items:
        offline_status = "Offline" if item.get("offline") else "Online"
        item_type = item.get("type")
        item_createdAt = item.get("createdAt")
        item_status = item.get("status", "Unknown")

        billing_id = item.get("billingId")
        if not billing_id:
            travelers = item.get("travelers", [])
            billing_id = next((traveler.get("billingId") for traveler in travelers if traveler.get("billingId")), None)

        raison = "Unknown"
        if billing_id:
            try:
                billing_doc = col_billings.find_one({"_id": ObjectId(billing_id)}, {"raison": 1})
            except errors.InvalidId:
                billing_doc = col_billings.find_one({"_id": billing_id}, {"raison": 1})

            if billing_doc:
                raison = billing_doc.get("raison", "Unknown")

        # R√©cup√©rer les d√©tails des segments et le nombre de jambes (legs) avec `fetch_train_details`
        leg_details, nombre_leg = fetch_train_details(item.get("id"))

        # Initialisation de `basic_data`
        basic_data = {
            "ITEM_ID": item.get("id"),
            "IS_OFFLINE": offline_status,
            "CREATED_AT": item_createdAt,
            "TYPE": item_type,
            "STATUS": item_status,
            "NB_LEGS": nombre_leg,  # Utilisation de nombre_leg ici
            "BILLING_ID": billing_id,
            "ENTITY": raison
        }

        # Fusionner les donn√©es de base avec les d√©tails des jambes
        combined_data = {**basic_data, **leg_details}

        # Ajout au DataFrame
        data.append(combined_data)

    # Cr√©er le DataFrame
    df = pd.DataFrame(data)

    # Sauvegarder le DataFrame en CSV
    df.to_csv(f"csv/base/items_train_{id_soc}.csv", index=False)

    print(df.head())

def concat_and_remove_columns(id_soc):
    df = pd.read_csv(f"csv/base/items_train_{id_soc}.csv")
    ori_columns = [col for col in df.columns if col.startswith(tuple(f'{i}_ori_name' for i in range(2, 10)))]
    des_columns = [col for col in df.columns if col.startswith(tuple(f'{i}_des_name' for i in range(2, 10)))]
    ori_locationId_columns = [col for col in df.columns if col.startswith(tuple(f'{i}_ori_locationId' for i in range(2, 10)))]
    des_locationId_columns = [col for col in df.columns if col.startswith(tuple(f'{i}_des_locationId' for i in range(2, 10)))]

    # Concat√©ner les colonnes ori, des, locationId
    df['ori_concat'] = df[ori_columns].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)
    df['des_concat'] = df[des_columns].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)
    df['ori_locationId_concat'] = df[ori_locationId_columns].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)
    df['des_locationId_concat'] = df[des_locationId_columns].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)

    # Supprimer les colonnes inutiles
    columns_to_remove = ori_columns + des_columns + ori_locationId_columns+ des_locationId_columns
    df.drop(columns=columns_to_remove, inplace=True)
    df.to_csv(f"csv/res/train/temp_{id_soc}.csv", index=False)

def normalize_station_names(id_soc):
    df = pd.read_csv(f"csv/res/train/temp_{id_soc}.csv")
    # Liste des colonnes √† traiter
    name_columns = [col for col in df.columns if '_ori_name' in col or '_des_name' in col]

    # Dictionnaire des remplacements
    replacements = {
        "PARIS": "PARIS",
        "MONTPELLIER": "MONTPELLIER",
        "MARSEILLE": "MARSEILLE ST CHARLES",
        "BORDEAUX": "BORDEAUX",
        "STRASBOURG": "STRASBOURG",
        "LYON": "LYON"
    }

    # Fonction pour normaliser une entr√©e
    def normalize_name(name):
        if pd.isna(name):
            return name
        name_upper = name.upper()  # Mettre tout en majuscule pour ignorer la casse
        for keyword, replacement in replacements.items():
            if keyword in name_upper:
                return replacement
        return name  # Retourner le nom original s'il n'y a pas de correspondance

    # Appliquer la normalisation √† toutes les colonnes pertinentes
    for col in name_columns:
        df[col] = df[col].apply(normalize_name)

    df.to_csv(f"csv/res/train/norm_{id_soc}.csv", index=False)

def fill_missing_location_ids(id_soc):
    # Charger les fichiers CSV
    temp_df = pd.read_csv(f"csv/res/train/norm_{id_soc}.csv")
    metropolis_df = pd.read_csv("const/O&D Metropolis rail 2024 v In&Out.csv", delimiter=";")
    european_df = pd.read_csv("const/O&D European Rail 2024 v In&Out.csv", delimiter=";")

    # Liste des colonnes √† traiter
    location_columns = [
        '0_ori_locationId', '0_des_locationId', '1_ori_locationId', '1_des_locationId',
    ]

    # Liste des colonnes correspondantes pour les noms (origine et destination)
    name_columns = [
        '0_ori_name', '0_des_name', '1_ori_name', '1_des_name',
    ]

    # Fonction pour interroger les fichiers et remplir le locationId
    def lookup_location_id(name, ref_df):
        # Chercher dans 'Label of railway station of origin'
        origin_match = ref_df[ref_df['Label of railway station of origin'] == name]
        if not origin_match.empty:
            return origin_match['Railway station code NTS of origin'].values[0]
        else:
            # Chercher dans 'Label of railway station of destination'
            destination_match = ref_df[ref_df['Label of railway station of destination'] == name]
            if not destination_match.empty:
                return destination_match['Railway station code NTS of destination'].values[0]
        return "PAS TROUV√â"

    # Remplir les valeurs manquantes pour chaque paire (locationId, name)
    for loc_col, name_col in zip(location_columns, name_columns):
        for idx, row in temp_df.iterrows():
            if pd.isna(row[loc_col]) and pd.notna(row[name_col]):
                # Chercher dans le fichier Metropolis
                result = lookup_location_id(row[name_col], metropolis_df)
                if result == "PAS TROUV√â":
                    # Si pas trouv√© dans Metropolis, chercher dans le fichier European
                    result = lookup_location_id(row[name_col], european_df)
                temp_df.at[idx, loc_col] = result
    temp_df = temp_df[(temp_df['0_ori_locationId'] != "PAS TROUV√â") & (temp_df['0_des_locationId'] != "PAS TROUV√â")]
    df = temp_df
    def determine_zone(row):
        # V√©rifier si une des valeurs ne commence pas par 'FR'
        columns_to_check = ['0_ori_locationId', '0_des_locationId', '1_ori_locationId', '1_des_locationId']

        for col in columns_to_check:
            if pd.notna(row[col]) and not row[col].startswith("FR"):
                return "EUROPE"
        return "METRO"
    df['Zone'] = temp_df.apply(determine_zone, axis=1)

    # Cr√©er la colonne 'Zone' en appliquant la fonction determine_zone √† chaque ligne

    df.to_csv(f"csv/res/train/concat_{id_soc}.csv", index=False)

def process_locations(id_soc):
    # Charger le DataFrame
    df = pd.read_csv(f"csv/res/train/concat_{id_soc}.csv")
    rail_metro_df = pd.read_csv('const/O&D Metropolis rail 2024 v In&Out.csv', delimiter=";")
    rail_euro_df = pd.read_csv('const/O&D European Rail 2024 v In&Out.csv', delimiter=";")

    # Fonction pour d√©terminer les 'ori_location_rest' et 'des_location_rest'
    def determine_rest_locations(row):
        if row['0_ori_name'] == 'PARIS':
            return row['0_ori_locationId'], row['0_des_locationId']
        elif row['0_des_name'] == 'PARIS':
            return row['0_des_locationId'], row['0_ori_locationId']
        else:
            locations = sorted([row['0_ori_locationId'], row['0_des_locationId']])
            ori_name, des_name = sorted([row['0_ori_name'], row['0_des_name']])
            if ori_name == row['0_des_name']:
                return row['0_des_locationId'], row['0_ori_locationId']
            return locations[0], locations[1]

    # Cr√©er les colonnes 'ori_location_rest' et 'des_location_rest'
    df['ori_location_rest'], df['des_location_rest'] = zip(*df.apply(determine_rest_locations, axis=1))

    # Fonction pour interroger rail_metro_df si la zone est 'METRO'
    def query_metropolis(ori_location, des_location):
        ori_code, ori_area, ori_label, des_code, des_label, des_country = None, None, None, None, None, None

        ori_match = rail_metro_df[rail_metro_df['Railway station code NTS of origin'] == ori_location]
        if not ori_match.empty:
            ori_code = ori_match['Origin code'].values[0]
            ori_area = ori_match['Origin area'].values[0]
            ori_label = ori_match['Label city of origin '].values[0]

        des_match = rail_metro_df[rail_metro_df['Railway station code NTS of destination'] == des_location]
        if not des_match.empty:
            des_code = des_match['Destination code'].values[0]
            des_country = des_match['Country code of destination'].values[0]
            des_label = des_match['Label city of destination '].values[0]

        return ori_code, ori_label, ori_area, des_code, des_label, des_country

    # Fonction pour interroger rail_euro_df si la zone est 'EUROPE'
    def query_europe(ori_location, des_location):
        ori_code, ori_area, ori_label, des_code, des_label, des_country = None, None, None, None, None, None

        ori_match = rail_euro_df[rail_euro_df['Railway station code NTS of origin'] == ori_location]
        if not ori_match.empty:
            ori_code = ori_match['Origin code'].values[0]
            ori_area = ori_match['Origin area'].values[0]
            ori_label = ori_match['Label city of origin '].values[0]

        des_match = rail_euro_df[rail_euro_df['Railway station code NTS of destination'] == des_location]
        if not des_match.empty:
            des_code = des_match['Destination code'].values[0]
            des_country = des_match['Country code of destination'].values[0]
            des_label = des_match['Label city of destination '].values[0]

        return ori_code, ori_label, ori_area, des_code, des_label, des_country

    # Appliquer les requ√™tes pour les lignes en fonction de la zone
    for idx, row in df.iterrows():
        if row['Zone'] == 'METRO':
            ori_code, ori_label, ori_area, des_code, des_label, des_country = query_metropolis(row['ori_location_rest'], row['des_location_rest'])
        elif row['Zone'] == 'EUROPE':
            ori_code, ori_label, ori_area, des_code, des_label, des_country = query_europe(row['ori_location_rest'], row['des_location_rest'])

        df.at[idx, 'ori_code'] = ori_code
        df.at[idx, 'ori_area'] = ori_area
        df.at[idx, 'des_code'] = des_code
        df.at[idx, 'des_country'] = des_country
        df.at[idx, 'ori_label'] = ori_label
        df.at[idx, 'des_label'] = des_label

    # Sauvegarder dans deux fichiers CSV
    df.to_csv(f"csv/res/train/raw_items_train_{id_soc}.csv", index=False)

def merge_extract_train(id_soc):
    df0 = pd.read_csv(f"csv/base/bills_{id_soc}.csv")
    df0 = df0[df0["type"] == "train"]
    df1 = pd.read_csv(f"csv/res/train/raw_items_train_{id_soc}.csv")
    df = pd.merge(df0, df1, on='ITEM_ID', how='inner')

    # Convert CREATED_AT to YYYY-MM and YYYY formats
    df['ISSUED_MONTH'] = pd.to_datetime(df['CREATED_AT']).dt.strftime('%Y%m')
    df['ODLIST'] = pd.to_datetime(df['CREATED_AT']).dt.strftime('%Y')
    df.to_csv(f"csv/res/train/merge_train_{id_soc}.csv")

def clean_train(id_soc,iata):
    df = pd.read_csv(f'csv/res/train/merge_train_{id_soc}.csv')
    df['O&D'] = df['ori_code'].fillna('') + " " + df['des_code'].fillna('')
    grouped_columns = ['ISSUED_MONTH', 'ODLIST', 'O&D', 'ori_code', 'ori_label',
                       'des_code', 'des_label', 'des_country', 'Zone',"ori_area"]

    grouped_result = df.groupby(grouped_columns).agg(
        TOTAL_BILLED=('TOTAL_BILLED', 'sum'),
        NB_LEGS=('NB_LEGS', 'sum')
    ).reset_index()
    grouped_result = grouped_result.sort_values(by=['O&D', 'ISSUED_MONTH'], ascending=[True, False])

    # Connexion en base pour recup OIN et NAME
    result = col_soc.find_one({"_id": ObjectId(f"{id_soc}")},
                              {"name": 1, "settings.flight.bluebizz": 1, "_id": 0})

    # Extraire les variables "name" et "settings.flight.bluebizz"
    name = result.get("name", None)
    bluebizz = result.get("settings", {}).get("flight", {}).get("bluebizz", None)
    bluebizz_value = bluebizz if bluebizz else "Bluebizz"

    # Remplir les colonnes du DataFrame
    grouped_result['REFERENCE AF'] = bluebizz_value
    grouped_result['RAISON SOCIALE'] = name
    grouped_result['IATA EMETTEUR'] = iata

    #rename col
    grouped_result = grouped_result.rename(columns={
        "ISSUED_MONTH": "DATE D'EMISSION",
        'ori_area': 'ZONE ORIGINE',
        'ori_code': 'CODE ORIGINE',
        'des_code': 'CODE DESTINATION',
        'ori_label': 'LIBELLE ORIGINE',
        'des_label': 'LIBELLE DESTINATION',
        'des_country': 'PAYS DESTINATION',
        'TOTAL_BILLED': 'CA',
        'NB_LEGS': 'NB O&D',
        'des_country': 'PAYS DESTINATION',
    })
    columns_order = [
        "O&D","Zone",'RAISON SOCIALE','REFERENCE AF',"DATE D'EMISSION",'CODE ORIGINE','LIBELLE ORIGINE', 'CODE DESTINATION',"PAYS DESTINATION",'LIBELLE DESTINATION',"IATA EMETTEUR",'ZONE ORIGINE','CA','NB O&D'
    ]

    grouped_result = grouped_result[columns_order]
    grouped_result = grouped_result.sort_values(by=["DATE D'EMISSION","O&D"], ascending=[False, True])

    metro_df = grouped_result[grouped_result['Zone'] == 'METRO']
    europe_df = grouped_result[grouped_result['Zone'] == 'EUROPE']

    # V√©rifier les correspondances pour la zone 'METRO'
    metro_df = metro_df[metro_df['Zone'] == 'METRO']
    metro_df['O&D_match'] = metro_df['O&D'].isin(rail_metro_df['O&D restitu√©'])
    # Filtrer les lignes o√π 'O&D' a une correspondance
    filtered_metro_df = metro_df[metro_df['O&D_match']].drop(columns=['O&D_match'])

    # V√©rifier les correspondances pour la zone 'EUROPE'
    europe_df = europe_df[europe_df['Zone'] == 'EUROPE']
    europe_df['O&D_match'] = europe_df['O&D'].isin(rail_euro_df['O&D restitu√©'])
    # Filtrer les lignes o√π 'O&D' a une correspondance
    filtered_europe_df = europe_df[europe_df['O&D_match']].drop(columns=['O&D_match'])


    # Saving the two DataFrames to CSV files
    filtered_metro_df.to_csv(f'csv/res/train/grouped_train_metro_{id_soc}.csv')
    filtered_europe_df.to_csv(f'csv/res/train/grouped_train_euro_{id_soc}.csv')

################ ü§ñ TRAITEMENT ü§ñ #####################

def calculate_rr_flight(id_soc) :

    # Charger les donn√©es
    df = pd.read_csv(f"csv/res/flight/grouped_flight_{id_soc}.csv")

    # Convertir 'DATE D\'EMISSION' en datetime et extraire l'ann√©e et le mois
    df['DATE D\'EMISSION'] = pd.to_datetime(df['DATE D\'EMISSION'], format='%Y%m')
    df['YEAR'] = df['DATE D\'EMISSION'].dt.year
    df['MONTH'] = df['DATE D\'EMISSION'].dt.month


    # Grouper par O&D, DATE D'EMISSION
    df_grouped = df.groupby(['O&D', 'DATE D\'EMISSION']).agg({
        'CA TOTAL INDUSTRIE': 'sum',
        'CA GROUPE AF KL': 'sum',
        'NB O&D GROUPE AF KL': 'sum',
        'NB O&D INDUSTRIE': 'sum'
    }).reset_index()


    # Filtrer les donn√©es pour "Online" uniquement
    df_online = df[df['TYPE DE VENTE'] == 'Online']

    df_online_grouped = df_online.groupby(['O&D', 'DATE D\'EMISSION']).agg({
        'CA TOTAL INDUSTRIE': 'sum',
        'CA GROUPE AF KL': 'sum',
        'NB O&D GROUPE AF KL': 'sum',
        'NB O&D INDUSTRIE': 'sum'
    }).reset_index()

    # Extraire l'ann√©e et le mois apr√®s le groupement
    df_grouped['YEAR'] = df_grouped['DATE D\'EMISSION'].dt.year
    df_grouped['MONTH'] = df_grouped['DATE D\'EMISSION'].dt.month
    df_online_grouped['YEAR'] = df_online_grouped['DATE D\'EMISSION'].dt.year
    df_online_grouped['MONTH'] = df_online_grouped['DATE D\'EMISSION'].dt.month

    # D√©finir le mois et l'ann√©e pr√©c√©dents
    current_year = pd.Timestamp.now().year
    current_month = pd.Timestamp.now().month

    # Si nous sommes en janvier, le mois pr√©c√©dent est d√©cembre de l'ann√©e derni√®re
    if current_month == 1:
        prev_month = 12
        prev_year = current_year - 1
    else:
        prev_month = current_month - 1
        prev_year = current_year

    # Filtrer les donn√©es du mois N et N-1
    df_n = df_grouped[(df_grouped['YEAR'] == current_year) & (df_grouped['MONTH'] == prev_month)].drop_duplicates(subset=['O&D']).set_index('O&D')
    df_n.to_csv("test.csv")

    # Prendre les donn√©es du mois pr√©c√©dent de l'ann√©e pr√©c√©dente
    df_n_1 = df_grouped[(df_grouped['YEAR'] == current_year - 1) & (df_grouped['MONTH'] == prev_month)].drop_duplicates(
        subset=['O&D']).set_index('O&D')

    df_online_n = df_online_grouped[(df_online_grouped['YEAR'] == current_year) & (df_online_grouped['MONTH'] == prev_month)].drop_duplicates(subset=['O&D']).set_index('O&D')

    # Calcul des RR pour chaque colonne
    columns_to_calculate = ['CA TOTAL INDUSTRIE', 'CA GROUPE AF KL', 'NB O&D GROUPE AF KL', 'NB O&D INDUSTRIE']
    df_rr_combined = pd.DataFrame(index=df_n.index)
    for column in columns_to_calculate:
        df_rr = df_n[[column]].join(df_n_1[[column]], lsuffix='_N', rsuffix='_N_1')
        df_rr[f'RR_{column}'] = (((df_rr[f'{column}_N'] / df_rr[f'{column}_N_1']) - 1) * 100).fillna(0).round(2)
        # Join N-1 and N values to df_rr_combined
        df_rr_combined = df_rr_combined.join(df_rr[[f'{column}_N', f'{column}_N_1', f'RR_{column}']])

    # Save the final CSV with N and N-1 values along with RR calculations
    # Calcul des ratios TP CA et TP OD
    df_rr_combined['TP_CA_AF_KL'] = (df_n['CA GROUPE AF KL'] / df_n['CA TOTAL INDUSTRIE']).round(2)
    df_rr_combined['TP_OD_AF_KL'] = (df_n['NB O&D GROUPE AF KL'] / df_n['NB O&D INDUSTRIE']).round(2)
    df_rr_combined['TP_CA_AF_KL_N_1'] = (df_n_1['CA GROUPE AF KL'] / df_n_1['CA TOTAL INDUSTRIE']).round(2)
    df_rr_combined['TP_OD_AF_KL_N_1'] = (df_n_1['NB O&D GROUPE AF KL'] / df_n_1['NB O&D INDUSTRIE']).round(2)

    # Calcul des nouvelles colonnes pour "Online"
    df_rr_combined['CA_AF_KL_ONLINE'] = df_online_n['CA GROUPE AF KL'].fillna(0)
    df_rr_combined['OD_AF_KL_ONLINE'] = df_online_n['NB O&D GROUPE AF KL'].fillna(0)
    df_rr_combined['CA_INDUSTRIE_ONLINE'] = df_online_n['CA TOTAL INDUSTRIE'].fillna(0)
    df_rr_combined['OD_INDUSTRIE_ONLINE'] = df_online_n['NB O&D INDUSTRIE'].fillna(0)

    # Calcul des ratios "Online"
    df_rr_combined['TP_CA_AF_KL_ONLINE'] = (df_online_n['CA GROUPE AF KL'] / df_online_n['CA TOTAL INDUSTRIE']).round(2)
    df_rr_combined['TP_OD_AF_KL_ONLINE'] = (df_online_n['NB O&D GROUPE AF KL'] / df_online_n['NB O&D INDUSTRIE']).round(2)

    # Calcul des √©volutions TP CA et TP OD
    df_rr_combined['EVOL_TP_CA'] = (df_rr_combined['TP_CA_AF_KL'] - df_rr_combined['TP_CA_AF_KL_N_1']).round(2)
    df_rr_combined['EVOL_TP_OD'] = (df_rr_combined['TP_OD_AF_KL'] - df_rr_combined['TP_OD_AF_KL_N_1']).round(2)

    #### CUMULS

    # Calculer les cumuls de janvier jusqu'au mois dernier pour 'CA TOTAL INDUSTRIE', 'CA GROUPE AF KL', etc.
    df_cumul = df_grouped[(df_grouped['YEAR'] == current_year) & (df_grouped['MONTH'] <= prev_month)]
    # Grouper par 'O&D' pour calculer les cumuls, sans √©craser les colonnes existantes
    df_cumul_grouped = df_cumul.groupby('O&D').agg({
        'CA TOTAL INDUSTRIE': 'sum',
        'CA GROUPE AF KL': 'sum',
        'NB O&D GROUPE AF KL': 'sum',
        'NB O&D INDUSTRIE': 'sum'
    }).reset_index()

    # Renommer les colonnes pour indiquer qu'il s'agit des cumuls
    df_cumul_grouped.rename(columns={
        'CA TOTAL INDUSTRIE': 'cumul_CA_TOTAL_INDUSTRIE',
        'CA GROUPE AF KL': 'cumul_CA_GROUPE_AF_KL',
        'NB O&D GROUPE AF KL': 'cumul_NB_O&D_GROUPE_AF_KL',
        'NB O&D INDUSTRIE': 'cumul_NB_O&D_INDUSTRIE'
    }, inplace=True)



    # Calcul des ratios TP_CA et TP_OD pour les cumuls
    df_cumul_grouped['cumul_TP_CA_AF_KL'] = (df_cumul_grouped['cumul_CA_GROUPE_AF_KL'] / df_cumul_grouped['cumul_CA_TOTAL_INDUSTRIE']).round(2)
    df_cumul_grouped['cumul_TP_OD_AF_KL'] = (df_cumul_grouped['cumul_NB_O&D_GROUPE_AF_KL'] / df_cumul_grouped['cumul_NB_O&D_INDUSTRIE']).round(2)

    # Calcul des cumuls RR pour 'CA TOTAL INDUSTRIE' et 'NB O&D INDUSTRIE'
    df_cumul_grouped['cumul_RR_CA TOTAL INDUSTRIE'] = (df_cumul_grouped['cumul_CA_TOTAL_INDUSTRIE'].pct_change() * 100).round(2)
    df_cumul_grouped['cumul_RR_NB O&D INDUSTRIE'] = (df_cumul_grouped['cumul_NB_O&D_INDUSTRIE'].pct_change() * 100).round(2)

    # Calcul des cumuls RR pour 'CA GROUPE AF KL' et 'NB O&D GROUPE AF KL'
    df_cumul_grouped['cumul_RR_CA GROUPE AF KL'] = (df_cumul_grouped['cumul_CA_GROUPE_AF_KL'].pct_change() * 100).round(2)
    df_cumul_grouped['cumul_RR_NB O&D GROUPE AF KL'] = (df_cumul_grouped['cumul_NB_O&D_GROUPE_AF_KL'].pct_change() * 100).round(2)


    # Calcul des cumuls TP_CA_AF_KL et TP_OD_AF_KL pour les transactions "Online"
    df_online_cumul = df_online_grouped[(df_online_grouped['YEAR'] == current_year) & (df_online_grouped['MONTH'] < current_month)]
    df_online_cumul_grouped = df_online_cumul.groupby('O&D').agg({
        'CA TOTAL INDUSTRIE': 'sum',
        'CA GROUPE AF KL': 'sum',
        'NB O&D GROUPE AF KL': 'sum',
        'NB O&D INDUSTRIE': 'sum'
    }).reset_index()

    df_online_cumul_grouped['cumul_TP_CA_AF_KL_ONLINE'] = (df_online_cumul_grouped['CA GROUPE AF KL'] / df_online_cumul_grouped['CA TOTAL INDUSTRIE']).round(2)
    df_online_cumul_grouped['cumul_TP_OD_AF_KL_ONLINE'] = (df_online_cumul_grouped['NB O&D GROUPE AF KL'] / df_online_cumul_grouped['NB O&D INDUSTRIE']).round(2)


    # V√©rification de l'ann√©e en cours et du mois pr√©c√©dent
    current_year = pd.Timestamp.now().year
    current_month = pd.Timestamp.now().month

    # Calcul du mois pr√©c√©dent
    if current_month == 1:
        prev_month = 12  # Si c'est janvier, on passe √† d√©cembre
        prev_year = current_year - 1  # Et on soustrait 1 ann√©e
    else:
        prev_month = current_month - 1
        prev_year = current_year  # Pas de changement d'ann√©e si on est apr√®s janvier

    # V√©rification de l'ann√©e N-1 pour les cumuls
    cumul_prev_year = current_year - 1  # Ann√©e N-1 pour les cumuls de janvier √† mois courant N-1

    # Filtrer les donn√©es du mois N (mois dernier) et N-1 (m√™me mois, ann√©e pr√©c√©dente)
    df_n = df_grouped[(df_grouped['YEAR'] == current_year) & (df_grouped['MONTH'] == prev_month)]
    df_n_1 = df_grouped[(df_grouped['YEAR'] == prev_year) & (df_grouped['MONTH'] == prev_month)]

    # Filtrer les donn√©es pour les cumuls de janvier √† mois courant de l'ann√©e N-1
    df_cumul_n_1 = df_grouped[(df_grouped['YEAR'] == cumul_prev_year) & (df_grouped['MONTH'] <= prev_month)]

    # Grouper les donn√©es pour calculer les cumuls par O&D pour N-1
    df_cumul_n_1_grouped = df_cumul_n_1.groupby('O&D').agg({
        'CA TOTAL INDUSTRIE': 'sum',
        'CA GROUPE AF KL': 'sum',
        'NB O&D GROUPE AF KL': 'sum',
        'NB O&D INDUSTRIE': 'sum'
    }).reset_index()

    # Calculer le cumul TP_CA_AF_KL et TP_OD_AF_KL par O&D pour l'ann√©e N-1
    df_cumul_grouped['cumul_TP_CA_AF_KL_N_1'] = df_cumul_grouped['O&D'].map(
        df_cumul_n_1_grouped.set_index('O&D').apply(lambda row: (row['CA GROUPE AF KL'] / row['CA TOTAL INDUSTRIE']).round(2) if row['CA TOTAL INDUSTRIE'] > 0 else 0,
            axis=1))

    df_cumul_grouped['cumul_TP_OD_AF_KL_N_1'] = df_cumul_grouped['O&D'].map(df_cumul_n_1_grouped.set_index('O&D').apply(lambda row: (row['NB O&D GROUPE AF KL'] / row['NB O&D INDUSTRIE']).round(2) if row['NB O&D INDUSTRIE'] > 0 else 0,
            axis=1))

    # Calcul des cumuls EVOL_TP_CA et EVOL_TP_OD en utilisant les cumuls
    df_cumul_grouped['cumul_EVOL_TP_CA'] = (df_cumul_grouped['cumul_TP_CA_AF_KL'] - df_cumul_grouped['cumul_TP_CA_AF_KL_N_1']).round(2)

    df_cumul_grouped['cumul_EVOL_TP_OD'] = (df_cumul_grouped['cumul_TP_OD_AF_KL'] - df_cumul_grouped['cumul_TP_OD_AF_KL_N_1']).round(2)
    # Joindre les r√©sultats "Online" √† df_cumul_grouped sans √©craser les colonnes
    df_cumul_grouped = df_cumul_grouped.join(df_online_cumul_grouped.set_index('O&D')[['cumul_TP_CA_AF_KL_ONLINE', 'cumul_TP_OD_AF_KL_ONLINE']], on='O&D')


    # Filtrer les donn√©es pour l'ann√©e N-1
    df_cumul_n_1 = df_grouped[(df_grouped['YEAR'] == current_year - 1) & (df_grouped['MONTH'] <= prev_month)]

    # Grouper par 'O&D' pour calculer les cumuls pour N-1
    df_cumul_n_1_grouped = df_cumul_n_1.groupby('O&D').agg({
        'CA TOTAL INDUSTRIE': 'sum',
        'CA GROUPE AF KL': 'sum',
        'NB O&D GROUPE AF KL': 'sum',
        'NB O&D INDUSTRIE': 'sum'
    }).reset_index()


    # Renommer les colonnes pour indiquer qu'il s'agit des cumuls N-1
    df_cumul_n_1_grouped.rename(columns={
        'CA TOTAL INDUSTRIE': 'cumul_CA_TOTAL_INDUSTRIE_N_1',
        'CA GROUPE AF KL': 'cumul_CA_GROUPE_AF_KL_N_1',
        'NB O&D GROUPE AF KL': 'cumul_NB_O&D_GROUPE_AF_KL_N_1',
        'NB O&D INDUSTRIE': 'cumul_NB_O&D_INDUSTRIE_N_1'
    }, inplace=True)

    # Calculer les cumuls pour les donn√©es "Online"
    df_online_cumul = df_online_grouped[
        (df_online_grouped['YEAR'] == current_year) & (df_online_grouped['MONTH'] <= prev_month)]
    df_online_cumul_grouped = df_online_cumul.groupby('O&D').agg({
        'CA TOTAL INDUSTRIE': 'sum',
        'CA GROUPE AF KL': 'sum',
        'NB O&D GROUPE AF KL': 'sum',
        'NB O&D INDUSTRIE': 'sum'
    }).reset_index()

    # Renommer les colonnes pour indiquer qu'il s'agit des cumuls online
    df_online_cumul_grouped.rename(columns={
        'CA TOTAL INDUSTRIE': 'cumul_CA_INDUSTRIE_ONLINE',
        'CA GROUPE AF KL': 'cumul_CA_AF_KL_ONLINE',
        'NB O&D GROUPE AF KL': 'cumul_OD_AF_KL_ONLINE',
        'NB O&D INDUSTRIE': 'cumul_OD_INDUSTRIE_ONLINE'
    }, inplace=True)

    df_cumul_n_1 = df_grouped[(df_grouped['YEAR'] == current_year - 1) & (df_grouped['MONTH'] <= prev_month)]
    df_cumul_n_1_grouped = df_cumul_n_1.groupby('O&D').agg({
        'CA TOTAL INDUSTRIE': 'sum',
        'CA GROUPE AF KL': 'sum',
        'NB O&D GROUPE AF KL': 'sum',
        'NB O&D INDUSTRIE': 'sum'
    }).reset_index()

    # Renommer les colonnes pour indiquer qu'il s'agit des cumuls pour N-1
    df_cumul_n_1_grouped.rename(columns={
        'CA TOTAL INDUSTRIE': 'cumul_CA_TOTAL_INDUSTRIE_N_1',
        'CA GROUPE AF KL': 'cumul_CA_GROUPE_AF_KL_N_1',
        'NB O&D GROUPE AF KL': 'cumul_NB_O&D_GROUPE_AF_KL_N_1',
        'NB O&D INDUSTRIE': 'cumul_NB_O&D_INDUSTRIE_N_1'
    }, inplace=True)

    # Fusionner les cumuls actuels et ceux de N-1 (√©craser les colonnes existantes)
    df_cumul_grouped = df_cumul_grouped.merge(df_cumul_n_1_grouped, on='O&D', how='left')

    # Fusionner les r√©sultats "Online" avec df_cumul_grouped (√©craser les colonnes existantes)
    df_cumul_grouped = df_cumul_grouped.merge(df_online_cumul_grouped, on='O&D', how='left')

    # Fusionner les r√©sultats de RR, TP et cumuls avec df_rr_combined (√©craser les colonnes existantes)
    df_final = pd.merge(df_rr_combined, df_cumul_grouped, on='O&D', how='outer')

    # Sauvegarder le fichier final avec RR, ratios et cumuls
    df_final.to_csv(f"csv/res/flight/grouped_flight_with_rr_{id_soc}.csv")

def merge_rr(id_soc):
    import pandas as pd

    # Load grouped_flight_rr and filtered_flight datasets
    grouped_flight_rr = pd.read_csv(f'csv/res/flight/grouped_flight_with_rr_{id_soc}.csv')

    # Initialize new columns for the required details
    grouped_flight_rr['HAUL_TYPE'] = None
    grouped_flight_rr['ORIGIN_AREA'] = None
    grouped_flight_rr['ANNEX_C'] = None
    grouped_flight_rr['LABEL_ORIGIN'] = None
    grouped_flight_rr['LABEL_DESTINATION'] = None
    grouped_flight_rr['COUNTRY_OF_DEST'] = None  # New column for 'Label country of destination'

    # Use .loc to match and assign all requested columns where O&D matches CONCATENATED_CITIES
    for idx in grouped_flight_rr.index:
        od_value = grouped_flight_rr.loc[idx, 'O&D']
        match = airports_df.loc[airports_df['O&D restitu√©'] == od_value]

        if not match.empty:
            grouped_flight_rr.loc[idx, 'HAUL_TYPE'] = match['Haul type'].values[0]
            grouped_flight_rr.loc[idx, 'ORIGIN_AREA'] = match['Origin area'].values[0]
            grouped_flight_rr.loc[idx, 'ANNEX_C'] = match['Annex C/ Out of Annex C 2023'].values[0]
            grouped_flight_rr.loc[idx, 'LABEL_ORIGIN'] = match['Label cities of origin'].values[0]
            grouped_flight_rr.loc[idx, 'LABEL_DESTINATION'] = match['Label cities of destination'].values[0]
            grouped_flight_rr.loc[idx, 'COUNTRY_OF_DEST'] = match['Country code of destination'].values[0]


    # Split 'O&D' into 'ORI' and 'DEST'
    grouped_flight_rr[['ORI', 'DEST']] = grouped_flight_rr['O&D'].str.split(n=1, expand=True)


    grouped_flight_rr.to_csv(f'csv/res/flight/grouped_flight_full_{id_soc}.csv', index=False)

def split_final(id_soc):
    import pandas as pd

    # Load the input CSV files
    df0 = pd.read_csv(f"csv/res/flight/grouped_flight_with_rr_{id_soc}.csv")
    df1 = pd.read_csv(f"csv/res/flight/grouped_flight_full_{id_soc}.csv")

    # Merge df0 and df1 based on the common columns
    df = pd.merge(df0, df1, how='inner', on=[col for col in df0.columns if col in df1.columns])
    # Define the desired column order
    columns_order = [
        'O&D', 'ANNEX_C', 'ORI', 'DEST', 'LABEL_ORIGIN', 'LABEL_DESTINATION', 'COUNTRY_OF_DEST',
        'HAUL_TYPE', 'ORIGIN_AREA', 'CA TOTAL INDUSTRIE_N', 'RR_CA TOTAL INDUSTRIE',
        'NB O&D INDUSTRIE_N', 'RR_NB O&D INDUSTRIE', 'CA GROUPE AF KL_N', 'RR_CA GROUPE AF KL',
        'NB O&D GROUPE AF KL_N', 'RR_NB O&D GROUPE AF KL', 'TP_CA_AF_KL', 'TP_CA_AF_KL_ONLINE',
        'EVOL_TP_CA', 'TP_OD_AF_KL', 'TP_OD_AF_KL_ONLINE', 'EVOL_TP_OD', 'cumul_CA_TOTAL_INDUSTRIE',
        'cumul_RR_CA TOTAL INDUSTRIE', 'cumul_NB_O&D_INDUSTRIE', 'cumul_RR_NB O&D INDUSTRIE',
        'cumul_CA_GROUPE_AF_KL', 'cumul_RR_CA GROUPE AF KL', 'cumul_NB_O&D_GROUPE_AF_KL',
        'cumul_RR_NB O&D GROUPE AF KL', 'cumul_TP_CA_AF_KL', 'cumul_TP_CA_AF_KL_ONLINE',
        'cumul_EVOL_TP_CA', 'cumul_TP_OD_AF_KL', 'cumul_TP_OD_AF_KL_ONLINE', 'cumul_EVOL_TP_OD'
    ]

    # Split the DataFrame based on the 'ANNEX_C' column
    df_split = dict(tuple(df.groupby('ANNEX_C')))

    # Iterate through the split DataFrames and save each part as a CSV file locally, reordering columns
    for key, df_part in df_split.items():
        # Reorder columns based on the predefined order
        df_part = df_part.reindex(columns=columns_order)

        # Create the filename based on the 'ANNEX_C' value
        filename = f"csv/OK/{key}.csv"

        # Save the DataFrame part with totals to a CSV file locally
        df_part.to_csv(filename, index=False)

    print("Files saved successfully with reordered columns for each split file!")

def aggreg_flight(id_soc,name_orga):
    df = pd.read_csv(f"csv/res/flight/grouped_flight_{id_soc}.csv")

    # Reorder the columns based on the order you provided
    columns_order = [
        'O&D', 'RAISON SOCIALE', 'REFERENCE AF', 'DATE D\'EMISSION',
        'CODE ORIGINE', 'LIBELLE ORIGINE', 'CODE DESTINATION', 'LIBELLE DESTINATION',
        'PERIMETRE', 'IATA EMETTEUR', 'TYPE DE VENTE', 'ZONE ORIGINE',
        'CA TOTAL INDUSTRIE', 'NB O&D INDUSTRIE', 'CA GROUPE AF KL', 'NB O&D GROUPE AF KL'
    ]
    # Reorder the DataFrame
    df = df[columns_order]
    df = df.sort_values(by=['O&D', 'DATE D\'EMISSION'], ascending = [True, False])

    # Save the result to a new CSV file
    df.to_excel(f"csv/OK/Aggregated-IATA_{name_orga}_air.xlsx",index = False)

def aggreg_train(id_soc,name_orga):
    df_euro = pd.read_csv(f"csv/res/train/grouped_train_euro_{id_soc}.csv")
    df_metro = pd.read_csv(f"csv/res/train/grouped_train_metro_{id_soc}.csv")


    # Define the desired column order
    columns_order = [
        'O&D', 'Zone', 'RAISON SOCIALE', 'REFERENCE AF', 'DATE D\'EMISSION',
        'CODE ORIGINE', 'LIBELLE ORIGINE', 'CODE DESTINATION', 'LIBELLE DESTINATION',
        'PAYS DESTINATION', 'IATA EMETTEUR', 'ZONE ORIGINE', 'CA', 'NB O&D'
    ]

    # Reorder the columns for df_euro
    df_euro = df_euro.reindex(columns=columns_order)

    # Reorder the columns for df_metro
    df_metro = df_metro.reindex(columns=columns_order)

    # Save the results to new CSV files
    df_euro.to_excel(f"csv/OK/Aggregated-IATA_{name_orga}_rail_euro.xlsx", index=False)
    df_metro.to_excel(f"csv/OK/Aggregated-IATA_{name_orga}_rail_metro.xlsx", index=False)

    print("Reorganization and saving completed for both df_euro and df_metro.")

def calculate_rr_train(id_soc, df_name):
    # Load the CSV file into a DataFrame
    df = pd.read_csv(f'csv/res/train/grouped_train_{df_name}_{id_soc}.csv')

    # Check if the DataFrame is empty
    if df.empty:
        print(f"The {df_name} dataset is empty. No calculations can be performed.")
        return

    # Convert 'DATE D'EMISSION' to datetime using the format '%Y%m'
    df["DATE D'EMISSION"] = pd.to_datetime(df["DATE D'EMISSION"], format='%Y%m')

    # Define current year and month
    current_year = pd.Timestamp.now().year
    current_month = pd.Timestamp.now().month

    # Determine previous month
    if current_month == 1:
        prev_month = 12
        prev_year = current_year - 1
    else:
        prev_month = current_month - 1
        prev_year = current_year

    # Group by the specified columns
    group_columns = ["O&D", "CODE ORIGINE", "CODE DESTINATION", "LIBELLE ORIGINE", "LIBELLE DESTINATION"]

    # Column name for O&D
    od_column = 'NB O&D'

    # For the previous month of this year (N)
    df_prev_month_n = df[(df["DATE D'EMISSION"].dt.year == current_year) &
                         (df["DATE D'EMISSION"].dt.month == prev_month)].groupby(group_columns).agg(
        CA_last_month_N=("CA", "sum"),
        OD_last_month_N=(od_column, "sum")
    ).reset_index()

    # For the previous month of last year (N-1)
    df_prev_month_n_1 = df[(df["DATE D'EMISSION"].dt.year == current_year - 1) &
                           (df["DATE D'EMISSION"].dt.month == prev_month)].groupby(group_columns).agg(
        CA_last_month_N_1=("CA", "sum"),
        OD_last_month_N_1=(od_column, "sum")
    ).reset_index()

    # For January to the previous month of this year (N)
    df_jan_to_prev_month_n = df[(df["DATE D'EMISSION"].dt.year == current_year) &
                                (df["DATE D'EMISSION"].dt.month <= prev_month)].groupby(group_columns).agg(
        CA_jan_to_last_month_N=("CA", "sum"),
        OD_jan_to_last_month_N=(od_column, "sum")
    ).reset_index()

    # For January to the previous month of last year (N-1)
    df_jan_to_prev_month_n_1 = df[(df["DATE D'EMISSION"].dt.year == current_year - 1) &
                                  (df["DATE D'EMISSION"].dt.month <= prev_month)].groupby(group_columns).agg(
        CA_jan_to_last_month_N_1=("CA", "sum"),
        OD_jan_to_last_month_N_1=(od_column, "sum")
    ).reset_index()

    # Merge all results into a single dataframe
    df_aggregated = pd.merge(df_prev_month_n, df_prev_month_n_1, on=group_columns, how='outer')
    df_aggregated = pd.merge(df_aggregated, df_jan_to_prev_month_n, on=group_columns, how='outer')
    df_aggregated = pd.merge(df_aggregated, df_jan_to_prev_month_n_1, on=group_columns, how='outer')

    # Round non-RR columns to integers
    cols_to_round = [
        'CA_last_month_N', 'OD_last_month_N',
        'CA_last_month_N_1', 'OD_last_month_N_1',
        'CA_jan_to_last_month_N', 'OD_jan_to_last_month_N',
        'CA_jan_to_last_month_N_1', 'OD_jan_to_last_month_N_1'
    ]
    df_aggregated[cols_to_round] = df_aggregated[cols_to_round].round(0)

    # Calculate the Rate of Change (RR) for CA and O&D
    df_aggregated['RR_CA'] = (((df_aggregated['CA_last_month_N'] / df_aggregated['CA_last_month_N_1']) - 1) * 100).round(2)
    df_aggregated['RR_OD'] = (((df_aggregated['OD_last_month_N'] / df_aggregated['OD_last_month_N_1']) - 1) * 100).round(2)

    # Calculate the Cumulative Rate of Change (RR CUMUL) for CA and O&D
    df_aggregated['cumul_RR_CA'] = (((df_aggregated['CA_jan_to_last_month_N'] / df_aggregated['CA_jan_to_last_month_N_1']) - 1) * 100).round(2)
    df_aggregated['cumul_RR_OD'] = (((df_aggregated['OD_jan_to_last_month_N'] / df_aggregated['OD_jan_to_last_month_N_1']) - 1) * 100).round(2)

    # Calculate the total values for each metric
    total_values = df_aggregated[cols_to_round].sum()

    # Calculate the Rate of Change (RR) using total values
    rr_od_last_month = ((total_values['OD_last_month_N'] / total_values['OD_last_month_N_1'] - 1) * 100).round(2)
    rr_ca_last_month = ((total_values['CA_last_month_N'] / total_values['CA_last_month_N_1'] - 1) * 100).round(2)
    rr_ca_jan_to_last_month = ((total_values['CA_jan_to_last_month_N'] / total_values['CA_jan_to_last_month_N_1'] - 1) * 100).round(2)
    rr_od_jan_to_last_month = ((total_values['OD_jan_to_last_month_N'] / total_values['OD_jan_to_last_month_N_1'] - 1) * 100).round(2)

    # Create a DataFrame for the total row
    total_row = pd.DataFrame([{
        'O&D': 'Total',
        'CODE ORIGINE': '',
        'CODE DESTINATION': '',
        'LIBELLE ORIGINE': '',
        'LIBELLE DESTINATION': '',
        'CA_last_month_N': total_values['CA_last_month_N'],
        'OD_last_month_N': total_values['OD_last_month_N'],
        'CA_last_month_N_1': total_values['CA_last_month_N_1'],
        'OD_last_month_N_1': total_values['OD_last_month_N_1'],
        'CA_jan_to_last_month_N': total_values['CA_jan_to_last_month_N'],
        'OD_jan_to_last_month_N': total_values['OD_jan_to_last_month_N'],
        'CA_jan_to_last_month_N_1': total_values['CA_jan_to_last_month_N_1'],
        'OD_jan_to_last_month_N_1': total_values['OD_jan_to_last_month_N_1'],
        'RR_CA': rr_ca_last_month,
        'RR_OD': rr_od_last_month,
        'cumul_RR_CA': rr_ca_jan_to_last_month,
        'cumul_RR_OD': rr_od_jan_to_last_month
    }])

    # Append the total row to the aggregated DataFrame
    df_aggregated = pd.concat([df_aggregated, total_row], ignore_index=True)

    # Save the aggregated DataFrame with the total row to CSV
    df_aggregated.to_csv(f'csv/OK/total_{df_name}.csv', index=False)

def total(id_soc):
    # Load the CSV file
    file_path = f'csv/res/flight/grouped_flight_full_{id_soc}.csv'
    df = pd.read_csv(file_path)

    # Reorganize columns: move columns ending with "_N_1" to the end
    columns_to_move = [col for col in df.columns if col.endswith('_N_1')]
    columns_remaining = [col for col in df.columns if col not in columns_to_move]
    new_column_order = columns_remaining + columns_to_move

    # Reorganize the dataframe
    df_reorganized = df[new_column_order]

    # Calculate the totals by grouping by the column 'ANNEX_C'
    grouped_totals = df_reorganized.groupby('ANNEX_C').sum(numeric_only=True).reset_index()

    # TP Ratios for cumulative columns
    grouped_totals['cumul_TP_CA_AF_KL'] = (
                grouped_totals['cumul_CA_GROUPE_AF_KL'] / grouped_totals['cumul_CA_TOTAL_INDUSTRIE']).round(2)
    grouped_totals['cumul_TP_CA_AF_KL_N_1'] = (
                grouped_totals['cumul_CA_GROUPE_AF_KL_N_1'] / grouped_totals['cumul_CA_TOTAL_INDUSTRIE_N_1']).round(2)
    grouped_totals['cumul_TP_OD_AF_KL'] = (
                grouped_totals['cumul_NB_O&D_GROUPE_AF_KL'] / grouped_totals['cumul_NB_O&D_INDUSTRIE']).round(2)
    grouped_totals['cumul_TP_OD_AF_KL_N_1'] = (
                grouped_totals['cumul_NB_O&D_GROUPE_AF_KL_N_1'] / grouped_totals['cumul_NB_O&D_INDUSTRIE_N_1']).round(2)

    # Evolution calculation for cumulative values
    grouped_totals['cumul_EVOL_TP_CA'] = (
                grouped_totals['cumul_TP_CA_AF_KL'] - grouped_totals['cumul_TP_CA_AF_KL_N_1']).round(2)
    grouped_totals['cumul_EVOL_TP_OD'] = (
                grouped_totals['cumul_TP_OD_AF_KL'] - grouped_totals['cumul_TP_OD_AF_KL_N_1']).round(2)



    # Calculate the subtotals RR for each ANNEX_C group
    grouped_totals['RR_CA GROUPE AF KL'] = (((grouped_totals['CA GROUPE AF KL_N'] / grouped_totals['CA GROUPE AF KL_N_1']) - 1) * 100).round(2)
    grouped_totals['RR_NB O&D GROUPE AF KL'] = (((grouped_totals['NB O&D GROUPE AF KL_N'] / grouped_totals['NB O&D GROUPE AF KL_N_1']) - 1) * 100).round(2)
    grouped_totals['RR_CA TOTAL INDUSTRIE'] = (((grouped_totals['CA TOTAL INDUSTRIE_N'] / grouped_totals['CA TOTAL INDUSTRIE_N_1']) - 1) * 100).round(2)
    grouped_totals['RR_NB O&D INDUSTRIE'] = (((grouped_totals['NB O&D INDUSTRIE_N'] / grouped_totals['NB O&D INDUSTRIE_N_1']) - 1) * 100).round(2)

    # Calculate TP ratios for each ANNEX_C group
    grouped_totals['TP_CA_AF_KL'] = (grouped_totals['CA GROUPE AF KL_N'] / grouped_totals['CA TOTAL INDUSTRIE_N']).round(2)
    grouped_totals['TP_CA_AF_KL_N_1'] = (grouped_totals['CA GROUPE AF KL_N_1'] / grouped_totals['CA TOTAL INDUSTRIE_N_1']).round(2)
    grouped_totals['TP_OD_AF_KL'] = (grouped_totals['NB O&D GROUPE AF KL_N'] / grouped_totals['NB O&D INDUSTRIE_N']).round(2)
    grouped_totals['TP_OD_AF_KL_N_1'] = (grouped_totals['NB O&D GROUPE AF KL_N_1'] / grouped_totals['NB O&D INDUSTRIE_N_1']).round(2)

    # Evolution calculation
    grouped_totals['EVOL_TP_CA'] = (grouped_totals['TP_CA_AF_KL'] - grouped_totals['TP_CA_AF_KL_N_1']).round(2)
    grouped_totals['EVOL_TP_OD'] = (grouped_totals['TP_OD_AF_KL'] - grouped_totals['TP_OD_AF_KL_N_1']).round(2)

    # -----------------------------------
    # Cumulative Calculations (Prefix: 'cumul_')
    # -----------------------------------

    # RR Calculations for cumulative columns
    grouped_totals['cumul_RR_CA GROUPE AF KL'] = (((grouped_totals['cumul_CA_GROUPE_AF_KL'] / grouped_totals['cumul_CA_GROUPE_AF_KL_N_1']) - 1) * 100).round(2)
    grouped_totals['cumul_RR_NB O&D GROUPE AF KL'] = (((grouped_totals['cumul_NB_O&D_GROUPE_AF_KL'] / grouped_totals['cumul_NB_O&D_GROUPE_AF_KL_N_1']) - 1) * 100).round(2)
    grouped_totals['cumul_RR_CA TOTAL INDUSTRIE'] = (((grouped_totals['cumul_CA_TOTAL_INDUSTRIE'] / grouped_totals['cumul_CA_TOTAL_INDUSTRIE_N_1']) - 1) * 100).round(2)
    grouped_totals['cumul_RR_NB O&D INDUSTRIE'] = (((grouped_totals['cumul_NB_O&D_INDUSTRIE'] / grouped_totals['cumul_NB_O&D_INDUSTRIE_N_1']) - 1) * 100).round(2)

    # TP Ratios for cumulative columns
    grouped_totals['cumul_TP_CA_AF_KL'] = (grouped_totals['cumul_CA_GROUPE_AF_KL'] / grouped_totals['cumul_CA_TOTAL_INDUSTRIE']).round(2)
    grouped_totals['cumul_TP_CA_AF_KL_N_1'] = (grouped_totals['cumul_CA_GROUPE_AF_KL_N_1'] / grouped_totals['cumul_CA_TOTAL_INDUSTRIE_N_1']).round(2)
    grouped_totals['cumul_TP_OD_AF_KL'] = (grouped_totals['cumul_NB_O&D_GROUPE_AF_KL'] / grouped_totals['cumul_NB_O&D_INDUSTRIE']).round(2)
    grouped_totals['cumul_TP_OD_AF_KL_N_1'] = (grouped_totals['cumul_NB_O&D_GROUPE_AF_KL_N_1'] / grouped_totals['cumul_NB_O&D_INDUSTRIE_N_1']).round(2)

    # Evolution calculation for cumulative values
    grouped_totals['cumul_EVOL_TP_CA'] = (grouped_totals['cumul_TP_CA_AF_KL'] - grouped_totals['cumul_TP_CA_AF_KL_N_1']).round(2)
    grouped_totals['cumul_EVOL_TP_OD'] = (grouped_totals['cumul_TP_OD_AF_KL'] - grouped_totals['cumul_TP_OD_AF_KL_N_1']).round(2)

    # Calculate global total across all 'ANNEX_C' groups for numeric columns
    global_totals_sum = grouped_totals.sum(numeric_only=True)

    # Recalculate TP, RR, and EVOL metrics for the global total
    global_totals_sum['RR_CA GROUPE AF KL'] = (((global_totals_sum['CA GROUPE AF KL_N'] / global_totals_sum['CA GROUPE AF KL_N_1']) - 1) * 100).round(2)
    global_totals_sum['RR_NB O&D GROUPE AF KL'] = (((global_totals_sum['NB O&D GROUPE AF KL_N'] / global_totals_sum['NB O&D GROUPE AF KL_N_1']) - 1) * 100).round(2)
    global_totals_sum['RR_CA TOTAL INDUSTRIE'] = (((global_totals_sum['CA TOTAL INDUSTRIE_N'] / global_totals_sum['CA TOTAL INDUSTRIE_N_1']) - 1) * 100).round(2)
    global_totals_sum['RR_NB O&D INDUSTRIE'] = (((global_totals_sum['NB O&D INDUSTRIE_N'] / global_totals_sum['NB O&D INDUSTRIE_N_1']) - 1) * 100).round(2)

    global_totals_sum['TP_CA_AF_KL'] = (global_totals_sum['CA GROUPE AF KL_N'] / global_totals_sum['CA TOTAL INDUSTRIE_N']).round(2)
    global_totals_sum['TP_CA_AF_KL_N_1'] = (global_totals_sum['CA GROUPE AF KL_N_1'] / global_totals_sum['CA TOTAL INDUSTRIE_N_1']).round(2)
    global_totals_sum['EVOL_TP_CA'] = (global_totals_sum['TP_CA_AF_KL'] - global_totals_sum['TP_CA_AF_KL_N_1']).round(2)
    global_totals_sum['TP_OD_AF_KL'] = (global_totals_sum['NB O&D GROUPE AF KL_N'] / global_totals_sum['NB O&D INDUSTRIE_N']).round(2)
    global_totals_sum['TP_OD_AF_KL_N_1'] = (global_totals_sum['NB O&D GROUPE AF KL_N_1'] / global_totals_sum['NB O&D INDUSTRIE_N_1']).round(2)
    global_totals_sum['EVOL_TP_OD'] = (global_totals_sum['TP_OD_AF_KL'] - global_totals_sum['TP_OD_AF_KL_N_1']).round(2)

    # Add a row for the global total

    global_totals_sum = grouped_totals.sum(numeric_only=True)
    global_totals_sum['ANNEX_C'] = 'Total GLOBAL'  # Add the "Total GLOBAL" value to the "ANNEX_C" column

    # Convert global_totals_sum to DataFrame and append it to the existing DataFrame
    global_totals_sum_df = pd.DataFrame(global_totals_sum).T
    grouped_totals = pd.concat([grouped_totals, global_totals_sum_df], ignore_index=True)

    output_path = 'csv/OK/totals_flight.csv'

    grouped_totals.to_csv(output_path)
import pandas as pd

def total_global():
    # Load the CSV file
    file_path = f'csv/OK/totals_flight.csv'
    df = pd.read_csv(file_path)
    df = df[df['ANNEX_C'] != 'Total GLOBAL']

    # Create a new row named "Total GLOBAL 2" with the specified calculations, rounded to 2 decimals
    total_global_2 = {
        'ANNEX_C': 'Total GLOBAL 2',
        'CA TOTAL INDUSTRIE_N': df['CA TOTAL INDUSTRIE_N'].sum().round(2),
        'RR_CA TOTAL INDUSTRIE': (((df['CA TOTAL INDUSTRIE_N'].sum() / df['CA TOTAL INDUSTRIE_N_1'].sum()) - 1) * 100).round(2),
        'CA GROUPE AF KL_N': df['CA GROUPE AF KL_N'].sum().round(2),
        'RR_CA GROUPE AF KL': (((df['CA GROUPE AF KL_N'].sum() / df['CA GROUPE AF KL_N_1'].sum()) - 1) * 100).round(2),
        'NB O&D GROUPE AF KL_N': df['NB O&D GROUPE AF KL_N'].sum().round(2),
        'RR_NB O&D GROUPE AF KL': (((df['NB O&D GROUPE AF KL_N'].sum() / df['NB O&D GROUPE AF KL_N_1'].sum()) - 1) * 100).round(2),
        'NB O&D INDUSTRIE_N': df['NB O&D INDUSTRIE_N'].sum().round(2),
        'RR_NB O&D INDUSTRIE': (((df['NB O&D INDUSTRIE_N'].sum() / df['NB O&D INDUSTRIE_N_1'].sum()) - 1) * 100).round(2),
        'TP_CA_AF_KL': (df['CA GROUPE AF KL_N'].sum() / df['CA TOTAL INDUSTRIE_N'].sum()).round(2),
        'TP_OD_AF_KL': (df['NB O&D GROUPE AF KL_N'].sum() / df['NB O&D INDUSTRIE_N'].sum()).round(2),
        'CA_AF_KL_ONLINE': df['CA_AF_KL_ONLINE'].sum().round(2),
        'OD_AF_KL_ONLINE': df['OD_AF_KL_ONLINE'].sum().round(2),
        'CA_INDUSTRIE_ONLINE': df['CA_INDUSTRIE_ONLINE'].sum().round(2),
        'OD_INDUSTRIE_ONLINE': df['OD_INDUSTRIE_ONLINE'].sum().round(2),
        'TP_CA_AF_KL_ONLINE': (df['CA_AF_KL_ONLINE'].sum() / df['CA_INDUSTRIE_ONLINE'].sum()).round(2),
        'TP_OD_AF_KL_ONLINE': (df['OD_AF_KL_ONLINE'].sum() / df['OD_INDUSTRIE_ONLINE'].sum()).round(2),
        'EVOL_TP_CA': ((df['CA GROUPE AF KL_N'].sum() / df['CA TOTAL INDUSTRIE_N'].sum()) / (df['CA GROUPE AF KL_N_1'].sum() / df['CA TOTAL INDUSTRIE_N_1'].sum())).round(2),
        'EVOL_TP_OD': ((df['NB O&D GROUPE AF KL_N'].sum() / df['NB O&D INDUSTRIE_N'].sum()) / (df['NB O&D GROUPE AF KL_N_1'].sum() / df['NB O&D INDUSTRIE_N_1'].sum())).round(2),
        'cumul_CA_TOTAL_INDUSTRIE': df['cumul_CA_TOTAL_INDUSTRIE'].sum().round(2),
        'cumul_CA_GROUPE_AF_KL': df['cumul_CA_GROUPE_AF_KL'].sum().round(2),
        'cumul_NB_O&D_GROUPE_AF_KL': df['cumul_NB_O&D_GROUPE_AF_KL'].sum().round(2),
        'cumul_NB_O&D_INDUSTRIE': df['cumul_NB_O&D_INDUSTRIE'].sum().round(2),
        'cumul_TP_CA_AF_KL': (df['cumul_CA_GROUPE_AF_KL'].sum() / df['cumul_CA_TOTAL_INDUSTRIE'].sum()).round(2),

        'cumul_TP_OD_AF_KL': (df['cumul_NB_O&D_GROUPE_AF_KL'].sum() / df['cumul_NB_O&D_INDUSTRIE'].sum()).round(2),

        'cumul_RR_CA TOTAL INDUSTRIE': (((df['cumul_CA_TOTAL_INDUSTRIE'].sum() / df['cumul_CA_TOTAL_INDUSTRIE_N_1'].sum()) - 1) * 100).round(2),

        'cumul_RR_NB O&D INDUSTRIE': (((df['cumul_NB_O&D_INDUSTRIE'].sum() / df['cumul_NB_O&D_INDUSTRIE_N_1'].sum()) - 1) * 100).round(2),

        'cumul_RR_CA GROUPE AF KL': (((df['cumul_CA_GROUPE_AF_KL'].sum() / df['cumul_CA_GROUPE_AF_KL_N_1'].sum()) - 1) * 100).round(2),
        'cumul_RR_NB O&D GROUPE AF KL': (((df['cumul_NB_O&D_GROUPE_AF_KL'].sum() / df['cumul_NB_O&D_GROUPE_AF_KL_N_1'].sum()) - 1) * 100).round(2),

        'cumul_EVOL_TP_CA': ((df['cumul_CA_GROUPE_AF_KL'].sum() / df['cumul_CA_TOTAL_INDUSTRIE'].sum()) - (df['cumul_CA_GROUPE_AF_KL_N_1'].sum() / df['cumul_CA_TOTAL_INDUSTRIE_N_1'].sum())).round(2),

        'cumul_EVOL_TP_OD': ((df['cumul_NB_O&D_GROUPE_AF_KL'].sum() / df['cumul_NB_O&D_INDUSTRIE'].sum()) - (df['cumul_NB_O&D_GROUPE_AF_KL_N_1'].sum() / df['cumul_NB_O&D_INDUSTRIE_N_1'].sum())).round(2),

        'cumul_TP_CA_AF_KL_ONLINE': (df['CA_AF_KL_ONLINE'].sum() / df['cumul_CA_TOTAL_INDUSTRIE'].sum()).round(2),
        'cumul_TP_OD_AF_KL_ONLINE': (df['OD_AF_KL_ONLINE'].sum() / df['cumul_OD_INDUSTRIE_ONLINE'].sum()).round(2),

        'cumul_RR_NB O&D INDUSTRIE': (((df['cumul_NB_O&D_INDUSTRIE'].sum() / df['cumul_NB_O&D_INDUSTRIE_N_1'].sum()) - 1) * 100).round(2),

        'cumul_RR_CA GROUPE AF KL': (((df['cumul_CA_GROUPE_AF_KL'].sum() / df['cumul_CA_GROUPE_AF_KL_N_1'].sum()) - 1) * 100).round(2),

        'cumul_RR_NB O&D GROUPE AF KL': (((df['cumul_NB_O&D_GROUPE_AF_KL'].sum() / df['cumul_NB_O&D_GROUPE_AF_KL_N_1'].sum()) - 1) * 100).round(2),

        'cumul_TP_CA_AF_KL_N_1': (((df['cumul_CA_GROUPE_AF_KL_N_1'].sum() / df['cumul_CA_TOTAL_INDUSTRIE_N_1'].sum()) - 1) * 100).round(2),

        'cumul_TP_OD_AF_KL_N_1': (((df['cumul_NB_O&D_GROUPE_AF_KL_N_1'].sum() / df['cumul_NB_O&D_INDUSTRIE_N_1'].sum()) - 1) * 100).round(2),

        'cumul_EVOL_TP_CA': ((df['cumul_CA_GROUPE_AF_KL'].sum() / df['cumul_CA_TOTAL_INDUSTRIE'].sum()).round(2))-(((df['cumul_CA_GROUPE_AF_KL_N_1'].sum() / df['cumul_CA_TOTAL_INDUSTRIE_N_1'].sum()) - 1) * 100).round(2),

        'cumul_EVOL_OD_CA': ((df['cumul_NB_O&D_GROUPE_AF_KL'].sum() / df['cumul_NB_O&D_INDUSTRIE'].sum()).round(2))-((((df['cumul_NB_O&D_GROUPE_AF_KL_N_1'].sum() / df['cumul_NB_O&D_INDUSTRIE_N_1'].sum()) - 1) * 100).round(2)),

        'cumul_TP_CA_AF_KL_ONLINE': (((df['cumul_CA_AF_KL_ONLINE'].sum() / df['cumul_CA_INDUSTRIE_ONLINE'].sum()) - 1) * 100).round(2),

        'cumul_TP_OD_AF_KL_ONLINE': (((df['cumul_OD_AF_KL_ONLINE'].sum() / df['cumul_OD_INDUSTRIE_ONLINE'].sum()) - 1) * 100).round(2),

        "cumul_CA_TOTAL_INDUSTRIE_N_1" : df['cumul_CA_TOTAL_INDUSTRIE_N_1'].sum().round(2),

        "cumul_CA_GROUPE_AF_KL_N_1" : df['cumul_CA_GROUPE_AF_KL_N_1'].sum().round(2),

        "cumul_NB_O&D_GROUPE_AF_KL_N_1" : df['cumul_NB_O&D_GROUPE_AF_KL_N_1'].sum().round(2),

        "cumul_NB_O&D_INDUSTRIE_N_1" : df['cumul_NB_O&D_INDUSTRIE_N_1'].sum().round(2),

        "cumul_CA_INDUSTRIE_ONLINE" : df['cumul_CA_INDUSTRIE_ONLINE'].sum().round(2),

        "cumul_CA_AF_KL_ONLINE" : df['cumul_CA_AF_KL_ONLINE'].sum().round(2),

        "cumul_OD_AF_KL_ONLINE" : df['cumul_OD_AF_KL_ONLINE'].sum().round(2),

        "cumul_OD_INDUSTRIE_ONLINE" : df['cumul_OD_INDUSTRIE_ONLINE'].sum().round(2),

    }

    # Append the "Total GLOBAL 2" row to the dataframe
    df = pd.concat([df, pd.DataFrame([total_global_2])], ignore_index=True)

    # Calculate [TP_CA_AF_KL_ONLINE] for IAC, IHAC, MAC, MHAC as [CA_AF_KL_ONLINE] / [CA_INDUSTRIE_ONLINE]
    df['TP_CA_AF_KL_ONLINE'] = (df['CA_AF_KL_ONLINE'] / df['CA_INDUSTRIE_ONLINE']).round(2)

    # Calculate [TP_OD_AF_KL_ONLINE] for IAC, IHAC, MAC, MHAC as [OD_AF_KL_ONLINE] / [OD_INDUSTRIE_ONLINE]
    df['TP_OD_AF_KL_ONLINE'] = (df['OD_AF_KL_ONLINE'] / df['OD_INDUSTRIE_ONLINE']).round(2)

    df['cumul_TP_CA_AF_KL_ONLINE'] = (((df['cumul_CA_AF_KL_ONLINE'] / df['cumul_CA_INDUSTRIE_ONLINE']) - 1) * 100).round(2)

    df['cumul_TP_OD_AF_KL_ONLINE'] = (((df['cumul_OD_AF_KL_ONLINE']/df['cumul_OD_INDUSTRIE_ONLINE'] ) - 1) * 100).round(2)

    # Save the updated DataFrame to CSV
    output_path = 'csv/OK/totals_flight.csv'
    df.to_csv(output_path, index=False)


def merge_total_flight():
    # Charger les 4 premi√®res lignes de `totals_flight.csv`
    totals_flight_path = 'csv/OK/totals_flight.csv'
    totals_flight = pd.read_csv(totals_flight_path, nrows=4).reset_index(drop=True)

    # Charger le fichier CSV appropri√© en fonction de la valeur de `ANNEX_C`
    for annex_c_value in totals_flight['ANNEX_C'].unique():
        # Charger le fichier correspondant √† la valeur de ANNEX_C
        file_name = f"csv/OK/{annex_c_value}.csv"
        try:
            df_annex = pd.read_csv(file_name)
        except FileNotFoundError:
            print(f"Fichier non trouv√© : {file_name}")
            continue

        # Filtrer la ligne dans totals_flight pour cette valeur de 'ANNEX_C'
        row_to_append = totals_flight[totals_flight['ANNEX_C'] == annex_c_value]

        if not row_to_append.empty:
            # Garder uniquement les colonnes √† partir de 'ORIGIN_AREA'
            try:
                start_column_index = df_annex.columns.get_loc('ORIGIN_AREA')
            except KeyError:
                print(f"'ORIGIN_AREA' non trouv√© dans les colonnes de {file_name}")
                continue

            columns_to_append = df_annex.columns[start_column_index:]

            # Trouver l'intersection des colonnes √† ajouter avec les colonnes disponibles dans row_to_append
            columns_to_append = [col for col in columns_to_append if col in row_to_append.columns]

            if not columns_to_append:
                print(f"Aucune colonne correspondante trouv√©e pour ANNEX_C {annex_c_value} dans totals_flight")
                continue

            # S'assurer que seules les colonnes appropri√©es sont utilis√©es pour l'ajout
            row_to_append = row_to_append[columns_to_append]

            # Ajouter la ligne filtr√©e au DataFrame
            df_annex = pd.concat([df_annex, row_to_append], ignore_index=True)

            # Sauvegarder le DataFrame mis √† jour dans le fichier CSV
            df_annex.to_csv(file_name, index=False)

            print(f"Ligne ajout√©e au fichier {file_name}")

from openpyxl.utils import get_column_letter
import pandas as pd
from openpyxl import load_workbook
from openpyxl.styles import PatternFill, Font, Alignment
from datetime import datetime
from openpyxl.styles.borders import Border, Side

def create_excel():
    # Load the CSV files into DataFrames
    iac_df = pd.read_csv('csv/OK/IAC.csv')
    ihac_df = pd.read_csv('csv/OK/IHAC.csv')
    mac_df = pd.read_csv('csv/OK/MAC.csv')
    mhac_df = pd.read_csv('csv/OK/MHAC.csv')

    # Drop the "O&D" and "ANNEX_C" columns if they exist in the DataFrames
    columns_to_exclude = ["O&D", "ANNEX_C"]

    for df in [iac_df, ihac_df, mac_df, mhac_df]:
        for column in columns_to_exclude:
            if column in df.columns:
                df.drop(columns=[column], inplace=True)

        # Drop rows where "cumul_CA_GROUPE_AF_KL" or "cumul_NB_O&D_GROUPE_AF_KL" are empty (NaN)
    # for df in [iac_df, ihac_df, mac_df, mhac_df]:
    #     df.dropna(subset=["cumul_CA_GROUPE_AF_KL", "cumul_NB_O&D_GROUPE_AF_KL"], inplace=True)

    # Load the existing Excel file
    excel_path = "const/template.xlsx"
    workbook = load_workbook(excel_path)

    # Select the worksheet "Etat 1.1"
    sheet_name = "Etat 1.1"
    if sheet_name in workbook.sheetnames:
        sheet = workbook[sheet_name]
    else:
        print(f"Sheet '{sheet_name}' does not exist in the workbook.")
        return

    # Unmerge all cells in the sheet to handle 'MergedCell' issues
    for merged_range in list(sheet.merged_cells.ranges):
        sheet.unmerge_cells(str(merged_range))

    # Define headers to be used for each table
    headers = [
        "ORI", "DEST", "DESTINATION", "ORIGINE", "CODE_PAYS", "TYPE_COURRIER", "ZONE_ORIGINE",
        "CA", "R/R", "O&D", "R/R", "CA", "R/R", "O&D", "R/R", "T/P CA", "T/P CA ONLINE",
        "EVOL T/P (CA)", "T/P O&D", "T/P O&D ONLINE", "EVOL T/P (O&D)", "CA", "R/R", "O&D", "R/R",
        "CA", "R/R", "O&D", "R/R", "T/P CA", "T/P CA ONLINE", "EVOL T/P (CA)", "T/P O&D",
        "T/P O&D ONLINE", "EVOL T/P (O&D)"
    ]

    # Define styles for headers, subtotals, top labels, date information, and borders
    header_fill = PatternFill(start_color="9370DB", end_color="9370DB", fill_type="solid")  # Purple fill
    header_font = Font(bold=True, color="FFFFFF")  # White font
    header_alignment = Alignment(horizontal="center", vertical="center")

    subtotal_fill = PatternFill(start_color="5c4292", end_color="5c4292", fill_type="solid")  # Red fill for subtotals
    subtotal_font = Font(bold=True, color="FFFFFF")  # White font for subtotals, bold
    subtotal_alignment = Alignment(horizontal="center", vertical="center")

    top_label_fill = PatternFill(start_color="9370DB", end_color="9370DB", fill_type="solid")  # Purple fill
    top_label_font = Font(bold=True, color="FFFFFF")  # White font for top labels
    top_label_alignment = Alignment(horizontal="center", vertical="center")

    date_fill = PatternFill(start_color="9370DB", end_color="9370DB", fill_type="solid")  # Purple fill
    date_font = Font(bold=True, color="FFFFFF")  # White font for date labels
    date_alignment = Alignment(horizontal="center", vertical="center")

    thin_border = Border(
        left=Side(border_style="thin", color="262626"),
        right=Side(border_style="thin", color="262626"),
        top=Side(border_style="thin", color="262626"),
        bottom=Side(border_style="thin", color="262626")
    )

    subtotals = ["SOUS TOTAL IAC", "SOUS TOTAL IHAC", "SOUS TOTAL MAC", "SOUS TOTAL MHAC"]

    # Get the current date information
    current_date = datetime.now()
    current_year = current_date.year
    last_month = current_date.month - 1 if current_date.month > 1 else 12
    last_month_name = datetime(1900, last_month, 1).strftime('%B')
    if last_month == 12:
        current_year -= 1  # Adjust the year if the last month is December from the previous year

    def write_dataframe_to_sheet(df, start_row, start_col, subtotal_text):
        sheet.merge_cells(f"H{start_row}:U{start_row}")
        last_month_label_cell = sheet["H" + str(start_row)]
        last_month_label_cell.value = f"{last_month_name} {current_year}"
        last_month_label_cell.fill = date_fill
        last_month_label_cell.font = date_font
        last_month_label_cell.alignment = date_alignment
        last_month_label_cell.border = thin_border

        # "Cumul de janvier √† {last month} {current year}" from V to AI
        sheet.merge_cells(f"V{start_row}:AI{start_row}")
        cumulative_label_cell = sheet["V" + str(start_row)]
        cumulative_label_cell.value = f"January {current_year} to {last_month_name} {current_year}"
        cumulative_label_cell.fill = date_fill
        cumulative_label_cell.font = date_font
        cumulative_label_cell.alignment = date_alignment
        cumulative_label_cell.border = thin_border

        # Write top labels above headers (one row below the date labels)
        label_ranges = [
            ("Industrie", "H", "K"),
            ("Groupe AF KL", "L", "O"),
            ("% GROUPE AF KL", "P", "U"),
            ("Industrie", "V", "Y"),
            ("Groupe AF KL", "Z", "AC"),
            ("% GROUPE AF KL", "AD", "AI")
        ]

        for label, start_col_letter, end_col_letter in label_ranges:
            sheet.merge_cells(f"{start_col_letter}{start_row + 1}:{end_col_letter}{start_row + 1}")
            top_label_cell = sheet[f"{start_col_letter}{start_row + 1}"]
            top_label_cell.value = label
            top_label_cell.fill = top_label_fill
            top_label_cell.font = top_label_font
            top_label_cell.alignment = top_label_alignment
            top_label_cell.border = thin_border


        # Write headers below the top labels
        for col_idx, header in enumerate(headers, start=start_col):
            cell = sheet.cell(row=start_row + 2, column=col_idx)
            cell.value = header
            cell.fill = header_fill
            cell.font = header_font
            cell.alignment = header_alignment
            cell.border = thin_border

        # Write data below headers
        for r_idx in range(len(df)):
            for c_idx in range(len(df.columns)):
                cell = sheet.cell(row=start_row + r_idx + 3, column=start_col + c_idx)
                cell.value = df.iloc[r_idx, c_idx]
                cell.border = thin_border

        # Write subtotal row (moved one row up)
        subtotal_row = start_row + len(df) + 2  # Subtotal row is now directly after the last data row
        start_merge_col = get_column_letter(start_col)
        end_merge_col = get_column_letter(start_col + 6)  # Merge columns A to G
        sheet.merge_cells(f"{start_merge_col}{subtotal_row}:{end_merge_col}{subtotal_row}")
        subtotal_cell = sheet.cell(row=subtotal_row, column=start_col)
        subtotal_cell.value = subtotal_text
        subtotal_cell.fill = subtotal_fill
        subtotal_cell.font = Font(bold=True, color="FFFFFF")  # White font for subtotal, bold
        subtotal_cell.alignment = subtotal_alignment
        subtotal_cell.border = thin_border

        for col_idx in range(start_col + 7, start_col + len(headers)):
            cell = sheet.cell(row=subtotal_row, column=col_idx)
            cell.font = Font(bold=True)  # Apply bold to the subtotal values
            cell.border = thin_border

    # Write the data to the worksheet
    start_row = 7
    start_col = 1

    # Write IAC data
    write_dataframe_to_sheet(iac_df, start_row, start_col, subtotals[0])
    # Calculate the new starting row for the next table
    start_row += len(iac_df) + 5

    # Write IHAC data
    write_dataframe_to_sheet(ihac_df, start_row, start_col, subtotals[1])
    start_row += len(ihac_df) + 5

    # Write MAC data
    write_dataframe_to_sheet(mac_df, start_row, start_col, subtotals[2])
    start_row += len(mac_df) + 5

    # Write MHAC data
    write_dataframe_to_sheet(mhac_df, start_row, start_col, subtotals[3])

    # Save the updated workbook
    updated_excel_path = 'csv/res/temp_final.xlsx'
    workbook.save(updated_excel_path)

    # Return the path of the updated file
    return updated_excel_path

def create_excel_train():
    # Load the CSV files into DataFrames
    euro_df = pd.read_csv('csv/OK/total_euro.csv')
    metro_df = pd.read_csv('csv/OK/total_metro.csv')

    # Drop the "O&D" column and columns ending with "_N_1"
    columns_to_exclude = ["O&D"] + [col for col in euro_df.columns if col.endswith("_N_1")]

    euro_df.drop(columns=columns_to_exclude, inplace=True, errors='ignore')
    metro_df.drop(columns=columns_to_exclude, inplace=True, errors='ignore')

    # Drop rows where "CA_jan_to_last_month_N" or "OD_jan_to_last_month_N" are empty (NaN)
    euro_df.dropna(subset=["CA_jan_to_last_month_N", "OD_jan_to_last_month_N"], inplace=True)
    metro_df.dropna(subset=["CA_jan_to_last_month_N", "OD_jan_to_last_month_N"], inplace=True)


    # Reorder columns as specified
    columns_order = [
        "CODE ORIGINE", "CODE DESTINATION", "LIBELLE ORIGINE", "LIBELLE DESTINATION",
        "CA_last_month_N", "RR_CA", "OD_last_month_N", "RR_OD",
        "CA_jan_to_last_month_N", "cumul_RR_CA", "OD_jan_to_last_month_N", "cumul_RR_OD"
    ]

    euro_df = euro_df[columns_order]
    metro_df = metro_df[columns_order]

    # Rename columns as specified
    rename_columns = {
        "CA_last_month_N": "CA",
        "CA_jan_to_last_month_N": "cumul_CA",
        "OD_last_month_N": "OD",
        "OD_jan_to_last_month_N": "cumul_OD"
    }

    euro_df.rename(columns=rename_columns, inplace=True)
    metro_df.rename(columns=rename_columns, inplace=True)

    # Load the existing Excel file
    excel_path = "csv/res/temp_final.xlsx"
    workbook = load_workbook(excel_path)

    # Select the worksheets "Etat 1.2" and "Etat 1.3"
    euro_sheet_name = "Etat 1.2"
    metro_sheet_name = "Etat 1.3"

    euro_sheet = workbook[euro_sheet_name] if euro_sheet_name in workbook.sheetnames else None
    metro_sheet = workbook[metro_sheet_name] if metro_sheet_name in workbook.sheetnames else None

    if not euro_sheet or not metro_sheet:
        print("One of the specified sheets does not exist in the workbook.")
        return

    # Define headers to be used for each table
    headers = list(euro_df.columns)

    # Define styles for headers, date information, and borders
    header_fill = PatternFill(start_color="9370DB", end_color="9370DB", fill_type="solid")  # Purple fill
    header_font = Font(bold=True, color="FFFFFF")  # White font
    header_alignment = Alignment(horizontal="center", vertical="center")

    date_fill = PatternFill(start_color="9370DB", end_color="9370DB", fill_type="solid")  # Purple fill
    date_font = Font(bold=True, color="FFFFFF")  # White font for date labels
    date_alignment = Alignment(horizontal="center", vertical="center")

    subtotal_fill = PatternFill(start_color="5c4292", end_color="5c4292",
                                fill_type="solid")  # Purple fill for subtotals
    subtotal_font = Font(bold=True, color="FFFFFF")  # White font for subtotals
    subtotal_alignment = Alignment(horizontal="center", vertical="center")
    thin_border = Border(
        left=Side(border_style="thin", color="262626"),
        right=Side(border_style="thin", color="262626"),
        top=Side(border_style="thin", color="262626"),
        bottom=Side(border_style="thin", color="262626")
    )

    # Get the current date information
    current_date = datetime.now()
    current_year = current_date.year
    last_month = current_date.month - 1 if current_date.month > 1 else 12
    last_month_name = datetime(1900, last_month, 1).strftime('%B')
    if last_month == 12:
        current_year -= 1  # Adjust the year if the last month is December from the previous year

    def write_dataframe_to_sheet(df, sheet, start_row, start_col, global_text):
        # Unmerge all cells in the sheet to handle any existing merged cells
        for merged_range in list(sheet.merged_cells.ranges):
            sheet.unmerge_cells(str(merged_range))

        for col_idx in range(5, 9):  # E to H for "Last month and year"
            cell = sheet.cell(row=start_row, column=col_idx)
            cell.border = thin_border

        for col_idx in range(9, 13):  # I to L for "Cumul de janvier √† {last month} {current year}"
            cell = sheet.cell(row=start_row, column=col_idx)
            cell.border = thin_border

        # Write the date labels above the top labels
        # "Last month and year" for the left part of the header
        sheet.merge_cells(f"E{start_row}:H{start_row}")
        last_month_label_cell = sheet["E" + str(start_row)]
        last_month_label_cell.value = f"{last_month_name} {current_year}"
        last_month_label_cell.fill = date_fill
        last_month_label_cell.font = date_font
        last_month_label_cell.alignment = date_alignment
        last_month_label_cell.border = thin_border

        # "Cumul de janvier √† {last month} {current year}" for the right part of the header
        sheet.merge_cells(f"I{start_row}:L{start_row}")
        cumulative_label_cell = sheet["I" + str(start_row)]
        cumulative_label_cell.value = f"January {current_year} to {last_month_name} {current_year}"
        cumulative_label_cell.fill = date_fill
        cumulative_label_cell.font = date_font
        cumulative_label_cell.alignment = date_alignment
        cumulative_label_cell.border = thin_border

        # Write headers below the date labels
        for col_idx, header in enumerate(headers, start=start_col):
            cell = sheet.cell(row=start_row + 1, column=col_idx)
            cell.value = header
            cell.fill = header_fill
            cell.font = header_font
            cell.alignment = header_alignment
            cell.border = thin_border

        # Write data below headers
        for r_idx in range(len(df)):
            for c_idx in range(len(df.columns)):
                cell = sheet.cell(row=start_row + r_idx + 2, column=start_col + c_idx)
                cell.value = df.iloc[r_idx, c_idx]
                cell.border = thin_border

        # Write global total row
        global_row = start_row + len(df) + 1  # Adjust row to move up by 1
        sheet.merge_cells(f"A{global_row}:D{global_row}")
        global_total_cell = sheet.cell(row=global_row, column=start_col)
        global_total_cell.value = global_text
        global_total_cell.fill = subtotal_fill
        global_total_cell.font = subtotal_font
        global_total_cell.alignment = subtotal_alignment
        global_total_cell.border = thin_border
        for col in range(start_col, start_col + len(headers)):
            cell = sheet.cell(row=global_row, column=col)
            cell.border = thin_border

        for col_idx in range(start_col + 4, start_col + len(headers)):
            cell = sheet.cell(row=global_row, column=col_idx)
            cell.font = Font(bold=True)  # Make the values bold
            cell.border = thin_border

    # Write euro data to "Etat 1.2"
    write_dataframe_to_sheet(metro_df, euro_sheet, start_row=4, start_col=1, global_text="GLOBAL RAIL")

    # Write metro data to "Etat 1.3"
    write_dataframe_to_sheet(euro_df, metro_sheet, start_row=4, start_col=1, global_text="GLOBAL RAIL")

    # Save the updated workbook
    updated_excel_path = 'csv/OK/temp_final.xlsx'
    workbook.save(updated_excel_path)

    # Return the path of the updated file
    return updated_excel_path

def extract_global():
    import pandas as pd

    # Load the provided CSV file
    csv_path = "csv/OK/totals_flight.csv"
    totals_flight = pd.read_csv(csv_path)

    # Extract the row for "Total GLOBAL"
    total_global_row = totals_flight[totals_flight['ANNEX_C'] == 'Total GLOBAL 2']

    # Define the required columns in the specified order
    required_columns = [
        "CA TOTAL INDUSTRIE_N", "RR_CA TOTAL INDUSTRIE", "NB O&D INDUSTRIE_N", "RR_NB O&D INDUSTRIE",
        "CA GROUPE AF KL_N", "RR_CA GROUPE AF KL", "NB O&D GROUPE AF KL_N", "RR_NB O&D GROUPE AF KL",
        "TP_CA_AF_KL", "TP_CA_AF_KL_ONLINE", "EVOL_TP_CA", "TP_OD_AF_KL", "TP_OD_AF_KL_ONLINE", "EVOL_TP_OD",
        "cumul_CA_TOTAL_INDUSTRIE", "cumul_RR_CA TOTAL INDUSTRIE", "cumul_NB_O&D_INDUSTRIE",
        "cumul_RR_NB O&D INDUSTRIE",
        "cumul_CA_GROUPE_AF_KL", "cumul_RR_CA GROUPE AF KL", "cumul_NB_O&D_GROUPE_AF_KL",
        "cumul_RR_NB O&D GROUPE AF KL",
        "cumul_TP_CA_AF_KL", "cumul_TP_CA_AF_KL_ONLINE", "cumul_EVOL_TP_CA", "cumul_TP_OD_AF_KL",
        "cumul_TP_OD_AF_KL_ONLINE", "cumul_EVOL_TP_OD"
    ]

    # Keep only the required columns from "Total GLOBAL"
    if not total_global_row.empty:
        total_global_filtered = total_global_row[required_columns]

        # Save the filtered row into a new CSV file
        output_path = "csv/OK/total_global.csv"
        total_global_filtered.to_csv(output_path)


def add_global(name_orga):
    import openpyxl
    from openpyxl.styles import Alignment
    import pandas as pd


    # Load the provided Excel and CSV files
    excel_path = "csv/OK/temp_final.xlsx"
    csv_path = f"csv/OK/total_global.csv"

    # Load the CSV file to get the "Total GLOBAL" row
    csv_data = pd.read_csv(csv_path)

    # Load the Excel workbook and the specific sheet
    workbook = openpyxl.load_workbook(excel_path)
    sheet = workbook["Etat 1.1"]

    # Find the row index of "SOUS TOTAL MHAC" in the Excel sheet
    sous_total_mhac_row = None
    for idx, row in enumerate(sheet.iter_rows(min_row=1, max_col=1, values_only=True), start=1):
        if row[0] == "SOUS TOTAL MHAC":
            sous_total_mhac_row = idx
            break

    # Insert "Total GLOBAL" two lines after "SOUS TOTAL MHAC"
    if sous_total_mhac_row is not None and not csv_data.empty:
        # Get the row to append and reset its index
        total_global_row = csv_data.reset_index(drop=True).iloc[0]

        # Prepare the insertion row number
        insert_row = sous_total_mhac_row + 2

        # Merge columns A to G for the "Total GLOBAL" label
        sheet.merge_cells(start_row=insert_row, start_column=1, end_row=insert_row, end_column=7)
        cell = sheet.cell(row=insert_row, column=1)
        cell.value = "Total GLOBAL"
        cell.alignment = Alignment(horizontal='center', vertical='center')

        # Write the rest of the values starting from column H
        # Writing all columns starting from H without changing their order
        for col_index, value in enumerate(total_global_row.iloc[1:], start=8):  # Start at column H, skip "ANNEX_C"
            sheet.cell(row=insert_row, column=col_index, value=value)

        # Save the updated workbook
        output_excel_path = f"csv/OK/{name_orga}_{last_month_name}_{current_year}.xlsx"
        workbook.save(output_excel_path)

    else:
        print("Total GLOBAL row not found in the CSV file or 'SOUS TOTAL MHAC' not found in the Excel sheet.")

import openpyxl
from openpyxl.utils import get_column_letter
from openpyxl.styles import PatternFill, Font, Alignment

def joli(name_orga):
    # Load the provided Excel workbook
    file_path = f"csv/OK/{name_orga}_September_2024.xlsx"
    workbook = openpyxl.load_workbook(file_path)

    workbook = openpyxl.load_workbook(file_path)

    # Define the white fill style for the added columns
    white_fill = PatternFill(start_color="FFFFFF", end_color="FFFFFF", fill_type="solid")

    # Define the styles for "TOTAL GLOBAL"
    header_fill = PatternFill(start_color="5C4292", end_color="5C4292",
                              fill_type="solid")  # Purple fill with better contrast
    header_font_white = Font(bold=True, color="FFFFFF")  # White font for header text
    header_font_black = Font(bold=True, color="000000")  # Black font for other cells in the row
    header_alignment = Alignment(horizontal="center", vertical="center")

    # Define border style for the row "Total GLOBAL"
    thin_border = Border(
        left=Side(style='thin'),
        right=Side(style='thin'),
        top=Side(style='thin'),
        bottom=Side(style='thin')
    )

    # Update the sheet "Etat 1.1"
    sheet = workbook["Etat 1.1"]

    # Iterate through merged cells to find "Total GLOBAL" and apply styles
    for merged_cell in sheet.merged_cells.ranges:
        top_left_cell = sheet.cell(row=merged_cell.min_row, column=merged_cell.min_col)
        if top_left_cell.value and "Total GLOBAL" in str(top_left_cell.value):
            # Apply styles to the merged cells (A to G) for "Total GLOBAL"
            for row in sheet.iter_rows(min_row=merged_cell.min_row, max_row=merged_cell.max_row,
                                       min_col=merged_cell.min_col, max_col=merged_cell.max_col):
                for cell in row:
                    cell.fill = header_fill
                    cell.font = header_font_white
                    cell.alignment = header_alignment

            # Apply bold style to the entire row of "Total GLOBAL" (from A to the last column), with black text for columns after G
            for col in range(1, sheet.max_column + 1):
                cell = sheet.cell(row=merged_cell.min_row, column=col)
                if col <= 7:  # Columns A to G should have white text
                    cell.font = header_font_white
                else:  # Other columns should have black text
                    cell.font = header_font_black

            # Apply border to the specified range of cells (A to U and W to AJ)
            for col in range(1, 22):  # Columns A to U (1 to 20)
                cell = sheet.cell(row=merged_cell.min_row, column=col)
                cell.border = thin_border

            for col in range(22, 36):  # Columns W to AJ (23 to 36)
                cell = sheet.cell(row=merged_cell.min_row, column=col)
                cell.border = thin_border

    # Create an empty column at "V" and shift contents to the right without breaking merged cells
    merged_cells = list(sheet.merged_cells.ranges)
    sheet.insert_cols(22)
    new_column_letter = get_column_letter(22)
    sheet.column_dimensions[new_column_letter].width = 0.56  # Set width to 2 mm
    for row in range(1, sheet.max_row + 1):
        sheet[f'{new_column_letter}{row}'].fill = white_fill

    # Update merged cell ranges to account for the inserted column
    sheet.merged_cells.ranges = []  # Clear existing merges
    for merged_cell in merged_cells:
        if merged_cell.min_col >= 22:
            new_min_col = merged_cell.min_col + 1
            new_max_col = merged_cell.max_col + 1
        else:
            new_min_col = merged_cell.min_col
            new_max_col = merged_cell.max_col
        new_range = f"{get_column_letter(new_min_col)}{merged_cell.min_row}:{get_column_letter(new_max_col)}{merged_cell.max_row}"
        sheet.merge_cells(new_range)

    # Update the sheets "Etat 1.2" and "Etat 1.3" by adding a column at "I"
    sheets_to_update = ["Etat 1.2", "Etat 1.3"]

    for sheet_name in sheets_to_update:
        sheet = workbook[sheet_name]
        merged_cells = list(sheet.merged_cells.ranges)
        sheet.insert_cols(9)
        new_column_letter = get_column_letter(9)
        sheet.column_dimensions[new_column_letter].width = 0.56  # Set width to 2 mm
        for row in range(1, sheet.max_row + 1):
            sheet[f'{new_column_letter}{row}'].fill = white_fill
        sheet.merged_cells.ranges = []  # Clear existing merges
        for merged_cell in merged_cells:
            if merged_cell.min_col >= 9:
                new_min_col = merged_cell.min_col + 1
                new_max_col = merged_cell.max_col + 1
            else:
                new_min_col = merged_cell.min_col
                new_max_col = merged_cell.max_col
            new_range = f"{get_column_letter(new_min_col)}{merged_cell.min_row}:{get_column_letter(new_max_col)}{merged_cell.max_row}"
            sheet.merge_cells(new_range)

    # Save the updated workbook
    output_path = f"csv/OK/{name_orga}_September_2024.xlsx"
    workbook.save(output_path)





